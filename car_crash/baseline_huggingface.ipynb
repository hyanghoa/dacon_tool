{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9543ec-32a9-4f95-b6fa-fbd262f3369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 03:05:02.141792: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-13 03:05:02.648893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-12.0/lib64/\n",
      "2023-03-13 03:05:02.648940: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-12.0/lib64/\n",
      "2023-03-13 03:05:02.648945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, AutoProcessor, AutoModel, VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "import pandas as pd\n",
    "import pytorchvideo\n",
    "import pytorchvideo.data\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd66b31-3e5a-4ad5-8e03-81b4bda38991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_df(method):\n",
    "#     train_df = pd.read_csv(Path(dataset_root_path, \"train.csv\"))\n",
    "    \n",
    "#     if method == \"crash\":\n",
    "#         train_df.loc[train_df[\"label\"]!=0, \"label\"] = 1\n",
    "    \n",
    "#     elif method == \"ego_involve\":\n",
    "#         train_df.drop(train_df[train_df[\"label\"]==0].index, inplace=True)\n",
    "#         train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==2)|(train_df[\"label\"]==3)|(train_df[\"label\"]==4)|(train_df[\"label\"]==5)|(train_df[\"label\"]==6), \"label\"] = 0 # yes\n",
    "#         train_df.loc[(train_df[\"label\"]==7)|(train_df[\"label\"]==8)|(train_df[\"label\"]==9)|(train_df[\"label\"]==10)|(train_df[\"label\"]==11)|(train_df[\"label\"]==12), \"label\"] = 1 # no\n",
    "        \n",
    "#     elif method == \"weather\":\n",
    "#         train_df.drop(train_df[(train_df[\"label\"]==0)].index, inplace=True)\n",
    "#         train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==2)|(train_df[\"label\"]==7)|(train_df[\"label\"]==8), \"label\"] = 0 # normal\n",
    "#         train_df.loc[(train_df[\"label\"]==3)|(train_df[\"label\"]==4)|(train_df[\"label\"]==9)|(train_df[\"label\"]==10), \"label\"] = 1 # snowy\n",
    "#         train_df.loc[(train_df[\"label\"]==5)|(train_df[\"label\"]==6)|(train_df[\"label\"]==11)|(train_df[\"label\"]==12), \"label\"] = 2 # rainy\n",
    "        \n",
    "#     elif method == \"timing\":\n",
    "#         train_df.drop(train_df[train_df[\"label\"]==0].index, inplace=True)\n",
    "#         train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==3)|(train_df[\"label\"]==5)|(train_df[\"label\"]==7)|(train_df[\"label\"]==9)|(train_df[\"label\"]==11), \"label\"] = 0 # day\n",
    "#         train_df.loc[(train_df[\"label\"]==2)|(train_df[\"label\"]==4)|(train_df[\"label\"]==6)|(train_df[\"label\"]==8)|(train_df[\"label\"]==10)|(train_df[\"label\"]==12), \"label\"] = 1 # night\n",
    "    \n",
    "#     train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])\n",
    "    \n",
    "#     return train_df, val_df\n",
    "\n",
    "# dataset_root_path = \"/home/djlee/deep/datasets/open\"\n",
    "\n",
    "# for method in [\"crash\", \"ego_involve\", \"weather\", \"timing\"]:\n",
    "#     train_df, val_df = get_df(method=method)\n",
    "\n",
    "#     for phase, df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "#         with open(Path(dataset_root_path, f\"{method}_{phase}.txt\"), \"w\") as f:\n",
    "#             for filename, label in df[[\"sample_id\", \"label\"]].values:\n",
    "#                 filename = f\"{filename}.mp4\"\n",
    "#                 f.write(f\"{filename} {label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ebc69f-a499-431e-af15-dbc3d240e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unnormalize_img(img):\n",
    "#     \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "#     img = (img * std) + mean\n",
    "#     img = (img * 255).astype(\"uint8\")\n",
    "#     return img.clip(0, 255)\n",
    "\n",
    "# def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "#     \"\"\"Prepares a GIF from a video tensor.\n",
    "    \n",
    "#     The video tensor is expected to have the following shape:\n",
    "#     (num_frames, num_channels, height, width).\n",
    "#     \"\"\"\n",
    "#     frames = []\n",
    "#     for video_frame in video_tensor:\n",
    "#         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "#         frames.append(frame_unnormalized)\n",
    "#     kargs = {\"duration\": 0.25}\n",
    "#     imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "#     return filename\n",
    "\n",
    "# def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "#     \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "#     video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "#     print(video_tensor.shape)\n",
    "#     gif_filename = create_gif(video_tensor, gif_name)\n",
    "#     return Image(filename=gif_filename)\n",
    "\n",
    "# sample_video = next(iter(train_dataset))\n",
    "# video_tensor = sample_video[\"video\"]\n",
    "# display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7190417d-fc27-4dbd-adda-6bcb28243626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9640ad-f983-4722-b374-21f3ad20ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        criterion = FocalLoss()\n",
    "        loss = criterion(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb971a64-176e-4f3d-8c9a-359d5e0f7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(method):\n",
    "    if method == \"crash\":\n",
    "        class_labels = [\"no\", \"yes\"]\n",
    "    elif method == \"ego_involve\":\n",
    "        class_labels = [\"yes\", \"no\"]\n",
    "    elif method == \"weather\":\n",
    "        class_labels = [\"normal\", \"snow\", \"rainy\"]\n",
    "    elif method == \"timing\":\n",
    "        class_labels = [\"day\", \"night\"]\n",
    "    \n",
    "    label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    model_ckpt = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "    image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "    model = VideoMAEForVideoClassification.from_pretrained(\n",
    "        model_ckpt,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "    )\n",
    "\n",
    "    dataset_root_path = \"/home/djlee/deep/datasets/open\"\n",
    "    mean = image_processor.image_mean\n",
    "    std = image_processor.image_std\n",
    "    if \"shortest_edge\" in image_processor.size:\n",
    "        height = width = image_processor.size[\"shortest_edge\"]\n",
    "    else:\n",
    "        height = image_processor.size[\"height\"]\n",
    "        width = image_processor.size[\"width\"]\n",
    "    resize_to = (height, width)\n",
    "\n",
    "    num_frames_to_sample = model.config.num_frames\n",
    "    sample_rate = 1\n",
    "    fps = 10\n",
    "    clip_duration = 5\n",
    "\n",
    "    # Training dataset transformations.\n",
    "    train_transform = Compose(\n",
    "        [\n",
    "            ApplyTransformToKey(\n",
    "                key=\"video\",\n",
    "                transform=Compose(\n",
    "                    [\n",
    "                        UniformTemporalSubsample(num_frames_to_sample),\n",
    "                        Lambda(lambda x: x / 255.0),\n",
    "                        Normalize(mean, std),\n",
    "                        ShortSideScale(256),\n",
    "                        RandomCrop(resize_to),\n",
    "                        RandomHorizontalFlip(p=0.5),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Validation and evaluation datasets' transformations.\n",
    "    val_transform = Compose(\n",
    "        [\n",
    "            ApplyTransformToKey(\n",
    "                key=\"video\",\n",
    "                transform=Compose(\n",
    "                    [\n",
    "                        UniformTemporalSubsample(num_frames_to_sample),\n",
    "                        Lambda(lambda x: x / 255.0),\n",
    "                        Normalize(mean, std),\n",
    "                        Resize(resize_to),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Training dataset.\n",
    "    train_dataset = pytorchvideo.data.Ucf101(\n",
    "        data_path=Path(dataset_root_path, f\"{method}_train.txt\"),\n",
    "        video_path_prefix=Path(dataset_root_path, \"train\"),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "        decode_audio=False,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    # Validation and evaluation datasets.\n",
    "    val_dataset = pytorchvideo.data.Ucf101(\n",
    "        data_path=Path(dataset_root_path, f\"{method}_val.txt\"),\n",
    "        video_path_prefix=Path(dataset_root_path, \"train\"),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "        decode_audio=False,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "    test_dataset = pytorchvideo.data.Ucf101(\n",
    "        data_path=Path(dataset_root_path, \"test_label.txt\"),\n",
    "        video_path_prefix=Path(dataset_root_path, \"test\"),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "        decode_audio=False,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "    print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)\n",
    "\n",
    "    model_name = model_ckpt.split(\"/\")[-1]\n",
    "    new_model_name = f\"{model_name}-{method}\"\n",
    "    num_epochs = 10\n",
    "    batch_size = 4\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        new_model_name,\n",
    "        remove_unused_columns=False,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        dataloader_num_workers=4,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        fp16=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    "    )\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "        result = metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "        result[\"f1\"] = f1_score(eval_pred.label_ids, predictions, average='macro')\n",
    "        return result\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        # permute to (num_frames, num_channels, height, width)\n",
    "        pixel_values = torch.stack(\n",
    "            [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "        )\n",
    "        labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "\n",
    "    train_results = trainer.train()\n",
    "    predict = trainer.predict(test_dataset)\n",
    "    return train_results, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcb39df-d2a0-4fe0-9fa6-08a89f9da58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158 540 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 21560\n",
      "  Num Epochs = 9223372036854775807\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5390\n",
      "  Number of trainable parameters = 86228738\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5390' max='5390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5390/5390 2:14:46, Epoch 9/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.959259</td>\n",
       "      <td>0.953078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.987602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>0.993776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>0.993825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.998148</td>\n",
       "      <td>0.997936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-540\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-540/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-540/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-540/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-1080\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-1080/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-1080/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-1080/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-1620\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-1620/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-1620/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-1620/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-2160\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-2160/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-2160/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-2160/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-2700\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-2700/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-2700/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-2700/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-3240\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-3240/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-3240/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-3240/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-3780\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-3780/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-3780/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-3780/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-4320\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-4320/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-4320/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-4320/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-4860\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-4860/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-4860/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-4860/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-crash/checkpoint-5390\n",
      "Configuration saved in videomae-base-finetuned-kinetics-crash/checkpoint-5390/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-crash/checkpoint-5390/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-crash/checkpoint-5390/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from videomae-base-finetuned-kinetics-crash/checkpoint-2700 (score: 1.0).\n",
      "***** Running Prediction *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "loading configuration file preprocessor_config.json from cache at /home/djlee/.cache/huggingface/hub/models--MCG-NJU--videomae-base-finetuned-kinetics/snapshots/4800870825aa2ee4430280b6a6a3f6cabf2d68b7/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n",
      "Image processor VideoMAEImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"VideoMAEFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"VideoMAEImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/djlee/.cache/huggingface/hub/models--MCG-NJU--videomae-base-finetuned-kinetics/snapshots/4800870825aa2ee4430280b6a6a3f6cabf2d68b7/config.json\n",
      "Model config VideoMAEConfig {\n",
      "  \"architectures\": [\n",
      "    \"VideoMAEForVideoClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"decoder_hidden_size\": 384,\n",
      "  \"decoder_intermediate_size\": 1536,\n",
      "  \"decoder_num_attention_heads\": 6,\n",
      "  \"decoder_num_hidden_layers\": 4,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"yes\",\n",
      "    \"1\": \"no\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"no\": 1,\n",
      "    \"yes\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"videomae\",\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_frames\": 16,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"tubelet_size\": 2,\n",
      "  \"use_mean_pooling\": true\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/djlee/.cache/huggingface/hub/models--MCG-NJU--videomae-base-finetuned-kinetics/snapshots/4800870825aa2ee4430280b6a6a3f6cabf2d68b7/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing VideoMAEForVideoClassification.\n",
      "\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732 183 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 7320\n",
      "  Num Epochs = 9223372036854775807\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1830\n",
      "  Number of trainable parameters = 86228738\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1830' max='1830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1830/1830 42:20, Epoch 9/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.039183</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.646214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.018504</td>\n",
       "      <td>0.928962</td>\n",
       "      <td>0.928268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>0.945355</td>\n",
       "      <td>0.944986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.023051</td>\n",
       "      <td>0.879781</td>\n",
       "      <td>0.879778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.907093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.041320</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.835471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.024346</td>\n",
       "      <td>0.928962</td>\n",
       "      <td>0.928655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.021909</td>\n",
       "      <td>0.912568</td>\n",
       "      <td>0.912440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.917876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.022912</td>\n",
       "      <td>0.923497</td>\n",
       "      <td>0.923312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-184\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-184/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-184/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-184/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-368\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-368/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-368/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-368/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-552\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-552/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-552/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-552/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-736\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-736/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-736/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-736/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-920\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-920/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-920/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-920/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-1104\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1104/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1104/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1104/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-1288\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1288/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1288/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1288/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-1472\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1472/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1472/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1472/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-1656\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1656/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1656/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1656/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-ego_involve/checkpoint-1830\n",
      "Configuration saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1830/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1830/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-ego_involve/checkpoint-1830/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from videomae-base-finetuned-kinetics-ego_involve/checkpoint-552 (score: 0.9449855699855699).\n",
      "***** Running Prediction *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "loading configuration file preprocessor_config.json from cache at /home/djlee/.cache/huggingface/hub/models--MCG-NJU--videomae-base-finetuned-kinetics/snapshots/4800870825aa2ee4430280b6a6a3f6cabf2d68b7/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n",
      "Image processor VideoMAEImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"VideoMAEFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"VideoMAEImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/djlee/.cache/huggingface/hub/models--MCG-NJU--videomae-base-finetuned-kinetics/snapshots/4800870825aa2ee4430280b6a6a3f6cabf2d68b7/config.json\n",
      "Model config VideoMAEConfig {\n",
      "  \"architectures\": [\n",
      "    \"VideoMAEForVideoClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"decoder_hidden_size\": 384,\n",
      "  \"decoder_intermediate_size\": 1536,\n",
      "  \"decoder_num_attention_heads\": 6,\n",
      "  \"decoder_num_hidden_layers\": 4,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"normal\",\n",
      "    \"1\": \"snow\",\n",
      "    \"2\": \"rainy\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"normal\": 0,\n",
      "    \"rainy\": 2,\n",
      "    \"snow\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"videomae\",\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_frames\": 16,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"tubelet_size\": 2,\n",
      "  \"use_mean_pooling\": true\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/djlee/.cache/huggingface/hub/models--MCG-NJU--videomae-base-finetuned-kinetics/snapshots/4800870825aa2ee4430280b6a6a3f6cabf2d68b7/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing VideoMAEForVideoClassification.\n",
      "\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732 183 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 7320\n",
      "  Num Epochs = 9223372036854775807\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1830\n",
      "  Number of trainable parameters = 86229507\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='369' max='1830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 369/1830 07:45 < 30:53, 0.79 it/s, Epoch 1.10/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.628152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.690087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-weather/checkpoint-184\n",
      "Configuration saved in videomae-base-finetuned-kinetics-weather/checkpoint-184/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-weather/checkpoint-184/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-weather/checkpoint-184/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to videomae-base-finetuned-kinetics-weather/checkpoint-368\n",
      "Configuration saved in videomae-base-finetuned-kinetics-weather/checkpoint-368/config.json\n",
      "Model weights saved in videomae-base-finetuned-kinetics-weather/checkpoint-368/pytorch_model.bin\n",
      "Image processor saved in videomae-base-finetuned-kinetics-weather/checkpoint-368/preprocessor_config.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:337] . unexpected pos 42506240 vs 42506128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/djlee/lib/python3.10/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/djlee/lib/python3.10/site-packages/torch/serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    667\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 668\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/107: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mego_involve\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiming\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     result[method] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 4\u001b[0m     result[method][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[method][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 153\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(method)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: pixel_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels}\n\u001b[1;32m    143\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    144\u001b[0m     model,\n\u001b[1;32m    145\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m    151\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m predict \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_results, predict\n",
      "File \u001b[0;32m~/anaconda3/envs/djlee/lib/python3.10/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/djlee/lib/python3.10/site-packages/transformers/trainer.py:1883\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1883\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1887\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/djlee/lib/python3.10/site-packages/transformers/trainer.py:2135\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/djlee/lib/python3.10/site-packages/transformers/trainer.py:2226\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2223\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SCALER_NAME))\n\u001b[1;32m   2224\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   2225\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2226\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m caught_warnings:\n\u001b[1;32m   2228\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SCHEDULER_NAME))\n",
      "File \u001b[0;32m~/anaconda3/envs/djlee/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/djlee/lib/python3.10/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:337] . unexpected pos 42506240 vs 42506128"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for method in [\"crash\", \"ego_involve\", \"weather\", \"timing\"]:\n",
    "    result[method] = {}\n",
    "    result[method][\"r\"], result[method][\"p\"] = run(method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f5fce-f23f-44a5-ad8d-16c21da8b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c6a79-764f-49d7-b142-fe25788e82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=Path(dataset_root_path, \"test_label.txt\"),\n",
    "    video_path_prefix=Path(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a416eb4-6ce6-4d16-b97d-532628079cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde66995-2f29-46b0-92fd-593aac37782d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
