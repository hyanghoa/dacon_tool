{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf22b8e8-71d3-4cce-bc70-4d90a159f491",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3c44a8-c46e-438d-811b-fd1ee3fadbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 21:52:22.299761: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 21:52:22.959453: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/djlee/anaconda3/envs/djlee/lib/python3.10/site-packages/cv2/../../lib64::/usr/local/cuda-12.0/lib64/\n",
      "2023-03-12 21:52:22.959591: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/djlee/anaconda3/envs/djlee/lib/python3.10/site-packages/cv2/../../lib64::/usr/local/cuda-12.0/lib64/\n",
      "2023-03-12 21:52:22.959597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import random\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models.video as vmodels\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "import hdf5plugin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import mmseg\n",
    "# from mmseg.apis import init_segmentor\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import pytorchvideo\n",
    "import pytorchvideo.data\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c7bc4-6190-4b30-98c9-5971c6a48593",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cc8f00-4caa-40e4-a280-2b04b9381b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"MViT\",\n",
    "    \"data_path\": \"/home/djlee/deep/datasets/open\",\n",
    "    \"video_length\": 50, # 10프레임 * 5초\n",
    "    \"image_size\": 224,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 1e-15,\n",
    "    \"max_lr\": 1e-3,\n",
    "    \"weight_decay\": 5e-2,\n",
    "    \"batch_size\": 2,\n",
    "    \"seed\": 42,\n",
    "    \"device\": torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n",
    "    \"num_workers\": 4,\n",
    "    \"accumulation_steps\": 4,\n",
    "    \"h5_name\": \"car.h5\",\n",
    "    \"methods\": [\"crash\", \"ego_involve\", \"weather\", \"timing\"],\n",
    "    \"early_stop\": 10,\n",
    "}\n",
    "now = datetime.datetime.now()\n",
    "config[\"save_path\"] = Path(f\"results/{now}\")\n",
    "config[\"save_path\"].mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf508a-64e1-4231-83b0-9a286f547424",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08cc917-56cb-48e4-9a32-4d8256e1998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 21:52:25,169 - {'model': 'MViT', 'data_path': '/home/djlee/deep/datasets/open', 'video_length': 50, 'image_size': 224, 'epochs': 10, 'learning_rate': 1e-15, 'max_lr': 0.001, 'weight_decay': 0.05, 'batch_size': 2, 'seed': 42, 'device': device(type='cuda'), 'num_workers': 4, 'accumulation_steps': 4, 'h5_name': 'car.h5', 'methods': ['crash', 'ego_involve', 'weather', 'timing'], 'early_stop': 10, 'save_path': PosixPath('results/2023-03-12 21:52:25.158308')}\n"
     ]
    }
   ],
   "source": [
    "# 로거 생성\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# 레벨 설정 - 'INFO' 레벨부터 출력\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 출력 포매팅 설정 - 시간, 로거이름, 로깅레벨, 메세지\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(message)s\")\n",
    "\n",
    "# 스트림 핸들러 설정 - 콘솔에 출력\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "# 파일 핸들러 설정 - 파일에 출력\n",
    "file_handler = logging.FileHandler(config[\"save_path\"].joinpath(\"result.log\"))\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb08d88-7b97-49b8-8900-61dd9676ffd5",
   "metadata": {},
   "source": [
    "## Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b47c63e-f531-4834-9bba-b3aff9d1a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0bbec7-38e1-4986-a919-65b8b83fdc6c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3315675f-b4f9-4279-b466-4a0d87f0cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_to_sample = 16\n",
    "mean = (0.45, 0.45, 0.45)\n",
    "std = (0.225, 0.225, 0.225)\n",
    "resize_to = (224, 224)\n",
    "\n",
    "# Training dataset transformations.\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(resize_to),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation and evaluation datasets' transformations.\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "    \n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    print(video_tensor.shape)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cc4ec3-8753-4725-b2ed-0e0cc824078f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_df(method):\n",
    "    train_df = pd.read_csv(os.path.join(config[\"data_path\"], \"train.csv\"))\n",
    "    \n",
    "    if method == \"crash\":\n",
    "        # 차량 충돌 여부 f1 90 이상\n",
    "        train_df.loc[train_df[\"label\"]!=0, \"label\"] = 1\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=config[\"seed\"], stratify=train_df['label'])\n",
    "    \n",
    "    elif method == \"crash_all\":\n",
    "        train_df.drop(train_df[train_df[\"label\"]==0].index, inplace=True)\n",
    "        train_df.loc[train_df[\"label\"]==1, \"label\"] = 0\n",
    "        train_df.loc[train_df[\"label\"]==2, \"label\"] = 1\n",
    "        train_df.loc[train_df[\"label\"]==3, \"label\"] = 2\n",
    "        train_df.loc[train_df[\"label\"]==4, \"label\"] = 3\n",
    "        train_df.loc[train_df[\"label\"]==5, \"label\"] = 4\n",
    "        train_df.loc[train_df[\"label\"]==6, \"label\"] = 5\n",
    "        train_df.loc[train_df[\"label\"]==7, \"label\"] = 6\n",
    "        train_df.loc[train_df[\"label\"]==8, \"label\"] = 7\n",
    "        train_df.loc[train_df[\"label\"]==9, \"label\"] = 8\n",
    "        train_df.loc[train_df[\"label\"]==10, \"label\"] = 9\n",
    "        train_df.loc[train_df[\"label\"]==11, \"label\"] = 10\n",
    "        train_df.loc[train_df[\"label\"]==12, \"label\"] = 11\n",
    "        \n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=config[\"seed\"], stratify=train_df['label'])\n",
    "        \n",
    "    elif method == \"ego_involve\":\n",
    "        # 차량 충돌 연관 여부 f1 68 이상\n",
    "        train_df.drop(train_df[train_df[\"label\"]==0].index, inplace=True)\n",
    "        train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==2)|(train_df[\"label\"]==3)|(train_df[\"label\"]==4)|(train_df[\"label\"]==5)|(train_df[\"label\"]==6), \"label\"] = 0 # yes\n",
    "        train_df.loc[(train_df[\"label\"]==7)|(train_df[\"label\"]==8)|(train_df[\"label\"]==9)|(train_df[\"label\"]==10)|(train_df[\"label\"]==11)|(train_df[\"label\"]==12), \"label\"] = 1 # no\n",
    "        \n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=config[\"seed\"], stratify=train_df['label'])\n",
    "        \n",
    "    elif method == \"weather\":\n",
    "        # 날씨 구분 f1 49 이상\n",
    "        train_df.drop(train_df[(train_df[\"label\"]==0)].index, inplace=True)\n",
    "        \n",
    "        # train_df.loc[train_df[\"label\"]==0, \"label\"] = -1\n",
    "        train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==2)|(train_df[\"label\"]==7)|(train_df[\"label\"]==8), \"label\"] = 0 # normal\n",
    "        train_df.loc[(train_df[\"label\"]==3)|(train_df[\"label\"]==4)|(train_df[\"label\"]==9)|(train_df[\"label\"]==10), \"label\"] = 1 # snowy\n",
    "        train_df.loc[(train_df[\"label\"]==5)|(train_df[\"label\"]==6)|(train_df[\"label\"]==11)|(train_df[\"label\"]==12), \"label\"] = 2 # rainy\n",
    "        # train_df.loc[train_df[\"label\"]==-1, \"label\"] = 3\n",
    "        # print(train_df[\"label\"].value_counts())\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=config[\"seed\"], stratify=train_df['label'])\n",
    "        # val_df.drop(val_df[val_df[\"label\"]==3].index, inplace=True)\n",
    "        \n",
    "    elif method == \"timing\":\n",
    "        # 낮/밤 구분 f1 90 이상\n",
    "        train_df.drop(train_df[train_df[\"label\"]==0].index, inplace=True)\n",
    "        train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==3)|(train_df[\"label\"]==5)|(train_df[\"label\"]==7)|(train_df[\"label\"]==9)|(train_df[\"label\"]==11), \"label\"] = 0 # day\n",
    "        train_df.loc[(train_df[\"label\"]==2)|(train_df[\"label\"]==4)|(train_df[\"label\"]==6)|(train_df[\"label\"]==8)|(train_df[\"label\"]==10)|(train_df[\"label\"]==12), \"label\"] = 1 # night\n",
    "        \n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=config[\"seed\"], stratify=train_df['label'])\n",
    "    \n",
    "    elif method == \"all\":\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=config[\"seed\"], stratify=train_df['label'])\n",
    "    \n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def apply_transform(image):\n",
    "    # Define the augmentations to apply to each video frame\n",
    "    transform = A.Compose([\n",
    "        A.Normalize(mean=(0.45, 0.45, 0.45), std=(0.225, 0.225, 0.225)),\n",
    "        # A.Crop (x_min=0, y_min=100, x_max=1280, y_max=620),\n",
    "        A.Resize(height=256, width=256, interpolation=cv2.INTER_AREA),\n",
    "        A.CenterCrop(height=config[\"image_size\"], width=config[\"image_size\"]),\n",
    "        # A.Crop (x_min=640-235, y_min=100, x_max=640+235, y_max=570), # 470\n",
    "        # A.Resize(height=int(config[\"image_size\"]/0.875), width=int(config[\"image_size\"]/0.875), interpolation=cv2.INTER_AREA),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.RandomSnow(p=1),\n",
    "        # A.RandomFog(p=1),\n",
    "        # A.RandomRain(p=1),\n",
    "        # A.RandomGamma(p=1),\n",
    "        # A.CLAHE(p=1),\n",
    "        # A.RandomBrightnessContrast(p=0.2),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    return transform(image=image)[\"image\"]\n",
    "\n",
    "def create_hdf5_dataset():\n",
    "\n",
    "    # Create the HDF5 file\n",
    "    with h5py.File(config[\"h5_name\"], 'w') as hf:\n",
    "    # with h5py.File(\"tmp.h5\", 'w') as hf:\n",
    "        \n",
    "        train_df = pd.read_csv(os.path.join(config[\"data_path\"], \"train.csv\"))\n",
    "        video_group = hf.create_group(\"videos\")\n",
    "        video_shape = (3, config[\"video_length\"], config[\"image_size\"], config[\"image_size\"])\n",
    "        \n",
    "        for idx, (sample_id, path, label) in enumerate(tqdm(train_df[[\"sample_id\", \"video_path\", \"label\"]].values)):\n",
    "            path = os.path.join(config[\"data_path\"], path[2:])\n",
    "            video = []\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            for i in range(config[\"video_length\"]):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = apply_transform(frame)\n",
    "                # plt.imshow(frame.moveaxis(0, -1))\n",
    "                # plt.show()\n",
    "                # break\n",
    "                video.append(frame)\n",
    "            video = np.stack(video).transpose(1, 0, 2, 3) # (channels, frames, height, width)\n",
    "            video_group.create_dataset(sample_id, data=video, dtype=np.float16)#, **hdf5plugin.Blosc2(cname='blosclz', clevel=5))\n",
    "        \n",
    "        for method in config[\"methods\"]:\n",
    "            train_df, val_df = get_df(method=method)\n",
    "            group = hf.create_group(method)\n",
    "            \n",
    "            for phase, df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "                labels_dataset = group.create_dataset(f\"{phase}/labels\", shape=(len(df),), dtype=np.uint8)\n",
    "                filenames_dataset = group.create_dataset(f\"{phase}/filenames\", shape=(len(df),), dtype=h5py.special_dtype(vlen=str))\n",
    "                \n",
    "                for idx, (sample_id, label) in enumerate(tqdm(df[[\"sample_id\", \"label\"]].values)):\n",
    "                    filenames_dataset[idx] = sample_id\n",
    "                    labels_dataset[idx] = int(label)\n",
    "\n",
    "# create_hdf5_dataset()\n",
    "# get_df(\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a049e7bd-b7a4-4832-98e5-1bbf606ff3ab",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f01482-2409-4d44-9e28-bd0a72154440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(config[\"h5_name\"], 'r') as hf:\n",
    "#     image = hf[\"videos/TRAIN_0000\"][:][:, 0]\n",
    "#     print(image.shape)\n",
    "#     print(image.transpose(1, 2, 0).shape)\n",
    "#     print(image.dtype)\n",
    "    \n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     plt.imshow(image.transpose(1, 2, 0).astype(np.float32))\n",
    "#     plt.show()\n",
    "# z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac97552d-6ff1-4372-98c7-0f80fce3f95e",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb2aa9-3555-4fb3-9b85-f8f3600562e0",
   "metadata": {},
   "source": [
    ">- ### CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff85b09-3c0b-45c1-b6de-01d18b0039e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hdf5_file, method, phase):\n",
    "        self.hdf5_file = h5py.File(hdf5_file, 'r')\n",
    "        self.length = self.hdf5_file[f\"{method}/{phase}/labels\"].shape[0]\n",
    "        self.method = method\n",
    "        self.phase = phase\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.hdf5_file[f\"{self.method}/{self.phase}/labels\"][index]\n",
    "        filename = self.hdf5_file[f\"{self.method}/{self.phase}/filenames\"][index].decode('utf-8')\n",
    "        video = self.hdf5_file[f\"videos/{filename}\"][:]\n",
    "        \n",
    "#         # Initialize an EncodedVideo helper class\n",
    "#         video = EncodedVideo.from_path(Path(config[\"data_path\"], \"train\", f\"{filename}.mp4\"))\n",
    "\n",
    "#         # Load the desired clip\n",
    "#         video = video.get_clip(start_sec=0, end_sec=5)\n",
    "        \n",
    "#         # Apply a transform to normalize the video input\n",
    "#         if self.phase == \"train\":\n",
    "#             video = train_transform(video)\n",
    "#         else:\n",
    "#             video = val_transform(video)\n",
    "\n",
    "#         # Move the inputs to the desired device\n",
    "#         video = video[\"video\"]\n",
    "        return video, label, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def _apply_transform(self, image, r):\n",
    "        transform_list = [\n",
    "            A.Resize(height=256, width=256, interpolation=cv2.INTER_AREA),\n",
    "            A.CenterCrop(height=config[\"image_size\"], width=config[\"image_size\"]),\n",
    "        ]\n",
    "        if r < 0.5:\n",
    "            transform_list.append(A.RandomSnow(p=1))\n",
    "        else:\n",
    "            transform_list.append(A.RandomRain(p=1))\n",
    "        transform_list.append(A.Normalize(mean=(0.45, 0.45, 0.45), std=(0.225, 0.225, 0.225)))\n",
    "        transform_list.append(ToTensorV2())\n",
    "        transform = A.Compose(transform_list)\n",
    "        return transform(image=image)[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ff6364-156d-4f85-bd02-7065228c4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, video_path_list, label_list, filename_list=None):\n",
    "        self.video_path_list = video_path_list\n",
    "        self.label_list = label_list\n",
    "        self.filename_list = filename_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.get_video(self.video_path_list[index])\n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            filename = self.filename_list[index]\n",
    "            return frames, label, filename\n",
    "        else:\n",
    "            return frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_path_list)\n",
    "    \n",
    "    def get_video(self, path):\n",
    "        path = os.path.join(config[\"data_path\"], path[2:])\n",
    "#         # Initialize an EncodedVideo helper class\n",
    "#         video = EncodedVideo.from_path(path)\n",
    "\n",
    "#         # Load the desired clip\n",
    "#         video = video.get_clip(start_sec=0, end_sec=5)\n",
    "        \n",
    "#         # Apply a transform to normalize the video input\n",
    "#         video = val_transform(video)\n",
    "\n",
    "#         # Move the inputs to the desired device\n",
    "#         video = video[\"video\"]\n",
    "        video = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        \n",
    "        for i in range(config[\"video_length\"]):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = apply_transform(frame)\n",
    "            video.append(frame)\n",
    "        video = np.stack(video).transpose(1, 0, 2, 3) # (channels, frames, height, width)\n",
    "        return video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b43ca6-a475-4140-aba6-aac4a4488a42",
   "metadata": {},
   "source": [
    ">- ### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3154806-8a13-42da-be56-e5d924d51e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=13):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.backbone = vmodels.r3d_18(weights=vmodels.R3D_18_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
    "        # self.classifier = nn.Linear(in_features=400, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2bb7832-c0fe-4c92-b47f-47fd82f9bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3D(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(S3D, self).__init__()\n",
    "        self.backbone = vmodels.slowfast_4x16(num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c5f2257-d702-4531-a87f-6687c7c7d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MViT(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(MViT, self).__init__()\n",
    "#         self.backbone = vmodels.mvit_v2_s(weights=vmodels.MViT_V2_S_Weights.DEFAULT)\n",
    "#         self.backbone.head = nn.Sequential(\n",
    "#             nn.Dropout(p=0.5, inplace=True),\n",
    "#             nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x = torch.cat([x[:, :, 0:1], x[:, :, 3:4], x[:, :, 6:7], x[:, :, 9:10], x[:, :, 12:13], x[:, :, 15:16], x[:, :, 18:19], x[:, :, 21:22], x[:, :, 24:25], x[:, :, 27:28], x[:, :, 30:31], x[:, :, 33:34], x[:, :, 36:37], x[:, :, 39:40], x[:, :, 42:43], x[:, :, -1:]], dim=2)\n",
    "#         # x = x[:, :, -16:]\n",
    "#         x = self.backbone(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fffb64a7-0206-4d14-8075-a0de988e9bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MViT(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MViT, self).__init__()\n",
    "        self.num_experts = 3\n",
    "        \n",
    "        self.backbone = vmodels.mvit_v2_s(weights=vmodels.MViT_V2_S_Weights.DEFAULT)\n",
    "        # for i, (name, params) in enumerate(self.backbone.named_parameters()):\n",
    "        #     if i < 160:\n",
    "        #         params.requires_grad = False\n",
    "            # print(i, name, params.requires_grad)\n",
    "        \n",
    "        blocks = nn.ModuleList([self.backbone.blocks.pop(-1) for _ in range(4)][::-1])\n",
    "        self.last_block = nn.ModuleList([blocks for _ in range(self.num_experts)])\n",
    "        self.norm = nn.ModuleList([self.backbone.norm for _ in range(self.num_experts)])\n",
    "        self.head = nn.ModuleList([nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "        ) for _ in range(self.num_experts)])\n",
    "        \n",
    "        \n",
    "        # self.expert_block = nn.ModuleList([\n",
    "        #     nn.Sequential(\n",
    "        #         last_block,\n",
    "        #         self.backbone.norm,\n",
    "        #         nn.Dropout(p=0.5, inplace=True),\n",
    "        #         nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "        #     )\n",
    "        # for _ in range(self.num_experts)])\n",
    "        # print(self.backbone)\n",
    "        del self.backbone.norm\n",
    "        del self.backbone.head\n",
    "        \n",
    "        # self.backbone.head = nn.Sequential(\n",
    "        #     nn.Dropout(p=0.5, inplace=True),\n",
    "        #     nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "        # )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([x[:, :, 0:1], x[:, :, 3:4], x[:, :, 6:7], x[:, :, 9:10], x[:, :, 12:13], x[:, :, 15:16], x[:, :, 18:19], x[:, :, 21:22], x[:, :, 24:25], x[:, :, 27:28], x[:, :, 30:31], x[:, :, 33:34], x[:, :, 36:37], x[:, :, 39:40], x[:, :, 42:43], x[:, :, -1:]], dim=2)\n",
    "        # x = x[:, :, -16:]\n",
    "        # x = self.backbone(x)\n",
    "        \n",
    "        # Convert if necessary (B, C, H, W) -> (B, C, 1, H, W)\n",
    "        x = self._unsqueeze(x, 5, 2)[0]\n",
    "        # patchify and reshape: (B, C, T, H, W) -> (B, embed_channels[0], T', H', W') -> (B, THW', embed_channels[0])\n",
    "        x = self.backbone.conv_proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # add positional encoding\n",
    "        x = self.backbone.pos_encoding(x)\n",
    "\n",
    "        # pass patches through the encoder\n",
    "        thw = (self.backbone.pos_encoding.temporal_size,) + self.backbone.pos_encoding.spatial_size\n",
    "        for block in self.backbone.blocks:\n",
    "            x, thw = block(x, thw)\n",
    "        \n",
    "        outs = []\n",
    "        for idx in range(self.num_experts):\n",
    "            outs.append(self._separate_part(x, thw, idx))\n",
    "        \n",
    "        final_out = torch.stack(outs, dim=1).mean(dim=1)\n",
    "        return {\n",
    "            \"output\": final_out, \n",
    "            \"logits\": torch.stack(outs, dim=1)\n",
    "        }\n",
    "    \n",
    "    def _separate_part(self, x, thw, idx):\n",
    "        for block in self.last_block[idx]:\n",
    "            x, thw = block(x, thw)\n",
    "        \n",
    "        x = self.norm[idx](x)\n",
    "\n",
    "        # classifier \"token\" as used by standard language architectures\n",
    "        x = x[:, 0]\n",
    "        x = self.head[idx](x)\n",
    "        return x\n",
    "    \n",
    "    def _unsqueeze(self, x: torch.Tensor, target_dim: int, expand_dim: int):\n",
    "        tensor_dim = x.dim()\n",
    "        if tensor_dim == target_dim - 1:\n",
    "            x = x.unsqueeze(expand_dim)\n",
    "        elif tensor_dim != target_dim:\n",
    "            raise ValueError(f\"Unsupported input dimension {x.shape}\")\n",
    "        return x, tensor_dim\n",
    "    \n",
    "# MViT(2)(torch.randn(4, 3, 50, 224, 224))\n",
    "# z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c2580d4-5c32-4664-9d3c-1acb26031488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin3d(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Swin3d, self).__init__()\n",
    "        self.backbone = vmodels.swin3d_s(weights=vmodels.Swin3D_S_Weights.DEFAULT)\n",
    "        self.backbone.head = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = x[:, :, -16:]\n",
    "        x = torch.cat([x[:, :, 0:1], x[:, :, 3:4], x[:, :, 6:7], x[:, :, 9:10], x[:, :, 12:13], x[:, :, 15:16], x[:, :, 18:19], x[:, :, 21:22], x[:, :, 24:25], x[:, :, 27:28], x[:, :, 30:31], x[:, :, 33:34], x[:, :, 36:37], x[:, :, 39:40], x[:, :, 42:43], x[:, :, -1:]], dim=2)\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f7db1c-f530-4017-bb4a-93c21cf329b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(HRNet, self).__init__()\n",
    "        config_file = \"fcn_hr18s_512x1024_40k_cityscapes.py\"\n",
    "        checkpoint = \"fcn_hr18s_512x1024_40k_cityscapes_20200601_014216-93db27d0.pth\"\n",
    "        self.backbone = init_segmentor(config_file, checkpoint=checkpoint, device=config[\"device\"]).backbone\n",
    "        self.classifier = nn.Linear(in_features=144, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 0]\n",
    "        x = self.backbone.forward(x)\n",
    "        x = F.adaptive_avg_pool2d(x[-1], 1)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4153a88-4e0a-4e4f-88a2-c835b1430eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherNet(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=2):\n",
    "        super(WeatherNet, self).__init__()\n",
    "        self.backbone = model = timm.create_model(backbone, num_classes=num_classes, pretrained=True) # drop_path_rate=args.drop_path,\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 0]\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9672c80-53c2-486a-9059-e2596939d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by positional_encoding in [pytorchvideo](https://github.com/facebookresearch/pytorchvideo/blob/f7e7a88a9a04b70cb65a564acfc38538fe71ff7b/pytorchvideo/layers/positional_encoding.py).\n",
    "Convert to pytorch version.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_3d_sincos_pos_embed(embed_dim: int,\n",
    "                            tube_shape: Tuple[int, int, int],\n",
    "                            stride,\n",
    "                            offset,\n",
    "                            kernel_size,\n",
    "                            cls_token: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get 3D sine-cosine positional embedding.\n",
    "    Args:\n",
    "        tube_shape: (t_size, grid_h_size, grid_w_size)\n",
    "        kernel_size:\n",
    "        offset:\n",
    "        stride:\n",
    "        embed_dim:\n",
    "        cls_token: bool, whether to contain CLS token\n",
    "    Returns:\n",
    "        (torch.Tensor): [t_size*grid_size*grid_size, embed_dim] or [1+t_size*grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 4 == 0\n",
    "    embed_dim_spatial = embed_dim // 3 * 2\n",
    "    embed_dim_temporal = embed_dim // 3\n",
    "\n",
    "    # spatial\n",
    "    grid_h_size = tube_shape[1]\n",
    "    grid_h = torch.arange(grid_h_size, dtype=torch.float)\n",
    "    grid_h = grid_h * stride[1] + offset[1] + kernel_size[1] // 2\n",
    "\n",
    "    grid_w_size = tube_shape[2]\n",
    "    grid_w = torch.arange(tube_shape[2], dtype=torch.float)\n",
    "    grid_w = grid_w * stride[2] + offset[2] + kernel_size[2] // 2\n",
    "    grid = torch.meshgrid(grid_w, grid_h, indexing='ij')\n",
    "    grid = torch.stack(grid, dim=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_h_size, grid_w_size])\n",
    "    pos_embed_spatial = get_2d_sincos_pos_embed_from_grid(embed_dim_spatial, grid)\n",
    "\n",
    "    # temporal\n",
    "    t_size = tube_shape[0]\n",
    "    grid_t = torch.arange(t_size, dtype=torch.float)\n",
    "    grid_t = grid_t * stride[0] + offset[0] + kernel_size[0] // 2\n",
    "    pos_embed_temporal = get_1d_sincos_pos_embed_from_grid(embed_dim_temporal, grid_t)\n",
    "\n",
    "    pos_embed_temporal = pos_embed_temporal[:, None, :]\n",
    "    pos_embed_temporal = torch.repeat_interleave(pos_embed_temporal, grid_h_size * grid_w_size, dim=1)\n",
    "    pos_embed_spatial = pos_embed_spatial[None, :, :]\n",
    "    pos_embed_spatial = torch.repeat_interleave(pos_embed_spatial, t_size, dim=0)\n",
    "\n",
    "    pos_embed = torch.cat([pos_embed_temporal, pos_embed_spatial], dim=-1)\n",
    "    pos_embed = pos_embed.reshape([-1, embed_dim])\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = torch.cat([torch.zeros([1, embed_dim]), pos_embed], dim=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim: int, grid_size: int, cls_token: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get 2D sine-cosine positional embedding.\n",
    "    Args:\n",
    "        grid_size: int of the grid height and width\n",
    "        cls_token: bool, whether to contain CLS token\n",
    "    Returns:\n",
    "        (torch.Tensor): [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = torch.arange(grid_size, dtype=torch.float)\n",
    "    grid_w = torch.arange(grid_size, dtype=torch.float)\n",
    "    grid = torch.meshgrid(grid_w, grid_h, indexing='ij')\n",
    "    grid = torch.stack(grid, dim=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = torch.cat([torch.zeros([1, embed_dim]), pos_embed], dim=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get 2D sine-cosine positional embedding from grid.\n",
    "    Args:\n",
    "        embed_dim: embedding dimension.\n",
    "        grid: positions\n",
    "    Returns:\n",
    "        (torch.Tensor): [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])\n",
    "\n",
    "    emb = torch.cat([emb_h, emb_w], dim=1)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get 1D sine-cosine positional embedding.\n",
    "    Args:\n",
    "        embed_dim: output dimension for each position\n",
    "        pos: a list of positions to be encoded: size (M,)\n",
    "    Returns:\n",
    "        (torch.Tensor): tensor of shape (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = torch.arange(embed_dim // 2, dtype=torch.float)\n",
    "    omega /= embed_dim / 2.0\n",
    "    omega = 1.0 / 10000 ** omega\n",
    "\n",
    "    pos = pos.reshape(-1)\n",
    "    out = torch.einsum(\"m,d->md\", pos, omega)\n",
    "\n",
    "    emb_sin = torch.sin(out)\n",
    "    emb_cos = torch.cos(out)\n",
    "\n",
    "    emb = torch.cat([emb_sin, emb_cos], dim=1)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb698c3c-cfc1-419e-b7cc-00572fa74f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Callable, Any\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision.models.vision_transformer import EncoderBlock\n",
    "from typing_extensions import OrderedDict\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Model Encoder for sequence to sequence translation.\n",
    "    Code from torch.\n",
    "    Move pos_embedding to TubeViT\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_layers: int,\n",
    "            num_heads: int,\n",
    "            hidden_dim: int,\n",
    "            mlp_dim: int,\n",
    "            dropout: float,\n",
    "            attention_dropout: float,\n",
    "            norm_layer: Callable[..., nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        torch._assert(x.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {x.shape}\")\n",
    "        return self.ln(self.layers(self.dropout(x)))\n",
    "\n",
    "\n",
    "class SparseTubesTokenizer(nn.Module):\n",
    "    def __init__(self, hidden_dim, kernel_sizes, strides, offsets):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.strides = strides\n",
    "        self.offsets = offsets\n",
    "\n",
    "        self.conv_proj_weight = nn.Parameter(torch.empty((self.hidden_dim, 3, *self.kernel_sizes[0])).normal_(),\n",
    "                                             requires_grad=True)\n",
    "\n",
    "        self.register_parameter('conv_proj_weight', self.conv_proj_weight)\n",
    "\n",
    "        self.conv_proj_bias = nn.Parameter(torch.zeros(len(self.kernel_sizes), self.hidden_dim), requires_grad=True)\n",
    "        self.register_parameter('conv_proj_bias', self.conv_proj_bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        n, c, t, h, w = x.shape  # CTHW\n",
    "        tubes = []\n",
    "        for i in range(len(self.kernel_sizes)):\n",
    "            if i == 0:\n",
    "                weight = self.conv_proj_weight\n",
    "            else:\n",
    "                weight = F.interpolate(self.conv_proj_weight, self.kernel_sizes[i], mode='trilinear')\n",
    "\n",
    "            tube = F.conv3d(\n",
    "                x[:, :, self.offsets[i][0]:, self.offsets[i][1]:, self.offsets[i][2]:],\n",
    "                weight,\n",
    "                bias=self.conv_proj_bias[i],\n",
    "                stride=self.strides[i],\n",
    "            )\n",
    "\n",
    "            tube = tube.reshape((n, self.hidden_dim, -1))\n",
    "\n",
    "            tubes.append(tube)\n",
    "\n",
    "        x = torch.cat(tubes, dim=-1)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        return x\n",
    "\n",
    "\n",
    "class TubeViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        video_shape: Union[List[int], np.ndarray],  # CTHW\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        representation_size=None,\n",
    "    ):\n",
    "        super(TubeViT, self).__init__()\n",
    "        self.video_shape = np.array(video_shape)  # CTHW\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_sizes = (\n",
    "            (8, 8, 8),\n",
    "            (16, 4, 4),\n",
    "            (4, 12, 12),\n",
    "            (1, 16, 16),\n",
    "        )\n",
    "\n",
    "        self.strides = (\n",
    "            (16, 32, 32),\n",
    "            (6, 32, 32),\n",
    "            (16, 32, 32),\n",
    "            (32, 16, 16),\n",
    "        )\n",
    "\n",
    "        self.offsets = (\n",
    "            (0, 0, 0),\n",
    "            (4, 8, 8),\n",
    "            (0, 16, 16),\n",
    "            (0, 0, 0),\n",
    "        )\n",
    "        self.sparse_tubes_tokenizer = SparseTubesTokenizer(self.hidden_dim, self.kernel_sizes, self.strides,\n",
    "                                                           self.offsets)\n",
    "\n",
    "        self.pos_embedding = self._generate_position_embedding()\n",
    "        self.pos_embedding = torch.nn.Parameter(self.pos_embedding, requires_grad=False)\n",
    "        self.register_parameter('pos_embedding', self.pos_embedding)\n",
    "\n",
    "        # Add a class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, self.hidden_dim), requires_grad=True)\n",
    "        self.register_parameter('class_token', self.class_token)\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "        )\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(self.hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, self.num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sparse_tubes_tokenizer(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        x = x[:, 0]\n",
    "\n",
    "        x = self.heads(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _calc_conv_shape(self, kernel_size, stride, offset) -> np.ndarray:\n",
    "        kernel_size = np.array(kernel_size)\n",
    "        stride = np.array(stride)\n",
    "        offset = np.array(offset)\n",
    "        output = np.ceil((self.video_shape[[1, 2, 3]] - offset - kernel_size + 1) / stride).astype(int)\n",
    "        return output\n",
    "\n",
    "    def _generate_position_embedding(self) -> torch.nn.Parameter:\n",
    "        position_embedding = [torch.zeros(1, self.hidden_dim)]\n",
    "\n",
    "        for i in range(len(self.kernel_sizes)):\n",
    "            tube_shape = self._calc_conv_shape(self.kernel_sizes[i], self.strides[i], self.offsets[i])\n",
    "            pos_embed = get_3d_sincos_pos_embed(\n",
    "                embed_dim=self.hidden_dim,\n",
    "                tube_shape=tube_shape,\n",
    "                kernel_size=self.kernel_sizes[i],\n",
    "                stride=self.strides[i],\n",
    "                offset=self.offsets[i],\n",
    "            )\n",
    "            position_embedding.append(pos_embed)\n",
    "\n",
    "        position_embedding = torch.cat(position_embedding, dim=0).contiguous()\n",
    "        return position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "406fe105-c4b1-422a-b395-0f660ce843b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HRNet(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(HRNet, self).__init__()\n",
    "#         self.backbone = timm.create_model('hrnet_w64', pretrained=True, num_classes=num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2444e4ce-9ae5-41bc-baa4-e8d50ba49a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mmseg.models import HRNet as mmhr\n",
    "# class HRNet(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(HRNet, self).__init__()\n",
    "#         extra = dict(\n",
    "#             stage1=dict(\n",
    "#                 num_modules=1,\n",
    "#                 num_branches=1,\n",
    "#                 block='BOTTLENECK',\n",
    "#                 num_blocks=(4, ),\n",
    "#                 num_channels=(64, )),\n",
    "#             stage2=dict(\n",
    "#                 num_modules=1,\n",
    "#                 num_branches=2,\n",
    "#                 block='BASIC',\n",
    "#                 num_blocks=(4, 4),\n",
    "#                 num_channels=(32, 64)),\n",
    "#             stage3=dict(\n",
    "#                 num_modules=4,\n",
    "#                 num_branches=3,\n",
    "#                 block='BASIC',\n",
    "#                 num_blocks=(4, 4, 4),\n",
    "#                 num_channels=(32, 64, 128)),\n",
    "#             stage4=dict(\n",
    "#                 num_modules=3,\n",
    "#                 num_branches=4,\n",
    "#                 block='BASIC',\n",
    "#                 num_blocks=(4, 4, 4, 4),\n",
    "#                 num_channels=(32, 64, 128, 256)))\n",
    "#         self.backbone = mmhr(extra, in_channels=3, pretrained=\"fcn_hr18s_512x1024_40k_cityscapes_20200601_014216-93db27d0.pth\")\n",
    "#         self.classifier = nn.Linear(in_features=256, out_features=num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)\n",
    "#         x = F.adaptive_avg_pool2d(x[-1], 1).flatten(1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c480290-f32e-4dc2-b1a3-a341500086ca",
   "metadata": {},
   "source": [
    ">- ### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e4ae647-7d2e-4c63-8642-52ca7703577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355f437-10f4-4eb5-ad16-d047dd37f722",
   "metadata": {},
   "source": [
    ">- ### Diverse Expert Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b12210de-95b8-47c3-a457-3e00250e951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiverseExpertLoss(nn.Module):\n",
    "    def __init__(self, method, cls_num_list=None, max_m=0.5, s=30, tau=2):\n",
    "        super().__init__()\n",
    "        self.base_loss = F.cross_entropy \n",
    "        if method == \"crash\":\n",
    "            cls_num_list = [0] * 2\n",
    "            cls_num_list[0] = 1783\n",
    "            cls_num_list[1] = 915\n",
    "            \n",
    "        elif method == \"ego_involve\":\n",
    "            cls_num_list = [0] * 2\n",
    "            cls_num_list[0] = 491\n",
    "            cls_num_list[1] = 424\n",
    "            \n",
    "        elif method == \"weather\":\n",
    "            cls_num_list = [0] * 3\n",
    "            cls_num_list[0] = 716\n",
    "            cls_num_list[1] = 129\n",
    "            cls_num_list[2] = 70\n",
    "            \n",
    "        elif method == \"timing\":\n",
    "            cls_num_list = [0] * 2\n",
    "            cls_num_list[0] = 808\n",
    "            cls_num_list[1] = 107\n",
    "            \n",
    "        prior = np.array(cls_num_list) / np.sum(cls_num_list)\n",
    "        self.prior = torch.tensor(prior).float().cuda()\n",
    "        self.C_number = len(cls_num_list)  # class number\n",
    "        self.s = s\n",
    "        self.tau = tau \n",
    "\n",
    "    def inverse_prior(self, prior): \n",
    "        value, idx0 = torch.sort(prior)\n",
    "        _, idx1 = torch.sort(idx0)\n",
    "        idx2 = prior.shape[0]-1-idx1 # reverse the order\n",
    "        inverse_prior = value.index_select(0,idx2)\n",
    "        \n",
    "        return inverse_prior\n",
    "\n",
    "    def forward(self, output_logits, target, extra_info=None):\n",
    "        if extra_info is None:\n",
    "            return self.base_loss(extra_info['output'], target)  # output_logits indicates the final prediction\n",
    "\n",
    "        loss = 0\n",
    "        \n",
    "        # Obtain logits from each expert  \n",
    "        expert1_logits = extra_info['logits'][:, 0]\n",
    "        expert2_logits = extra_info['logits'][:, 1] \n",
    "        expert3_logits = extra_info['logits'][:, 2]  \n",
    " \n",
    "        # Softmax loss for expert 1 \n",
    "        loss += self.base_loss(expert1_logits, target)\n",
    "        \n",
    "        # Balanced Softmax loss for expert 2 \n",
    "        expert2_logits = expert2_logits + torch.log(self.prior + 1e-9) \n",
    "        loss += self.base_loss(expert2_logits, target)\n",
    "        \n",
    "        # Inverse Softmax loss for expert 3\n",
    "        inverse_prior = self.inverse_prior(self.prior)\n",
    "        expert3_logits = expert3_logits + torch.log(self.prior + 1e-9) - self.tau * torch.log(inverse_prior+ 1e-9) \n",
    "        loss += self.base_loss(expert3_logits, target)\n",
    "   \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60b59a-baa2-4ffa-b243-6303041312c5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d572175-da09-4b1c-84e9-7323ffde9115",
   "metadata": {},
   "source": [
    ">- ### collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f700c905-5631-4e60-b161-81753bc29ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    filenames = [item[2] for item in batch]\n",
    "    print(type(labels[0]))\n",
    "    print(type(data[0]))\n",
    "    print(type(filenames[0]))\n",
    "    return torch.stack(data), torch.tensor(labels), filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8914191-c673-4b01-80c8-311d1edd779c",
   "metadata": {},
   "source": [
    ">- ### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f0e9d52-5588-4190-9a2d-01ef193b2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device, method):\n",
    "    model.to(device)\n",
    "    # criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "    # criterion = FocalLoss().to(device)\n",
    "    criterion = DiverseExpertLoss(method=method).to(device)\n",
    "    \n",
    "    best_epoch = 0\n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    early_stop = 0\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(1, config[\"epochs\"]+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        preds = []\n",
    "        trues = []\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(iter(train_loader))):\n",
    "            # optimizer.zero_grad()\n",
    "            \n",
    "            videos = batch[0].to(device=device, dtype=torch.float32, non_blocking=True)\n",
    "            labels = batch[1].to(device=device, dtype=torch.long, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(videos)\n",
    "                loss = criterion(output, labels, extra_info=output)\n",
    "                loss = loss / config[\"accumulation_steps\"]\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1) % config[\"accumulation_steps\"] == 0 or i + 1 == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            preds += output['output'].argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        train_f1_score = f1_score(trues, preds, average='macro')\n",
    "        # if scheduler is not None:\n",
    "            # scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_epoch = epoch\n",
    "            best_val_score = _val_score\n",
    "            best_model = deepcopy(model)\n",
    "            early_stop = 0\n",
    "        \n",
    "        logger.info(f'Method: [{method}] Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Train F1: [{train_f1_score:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}] Best F1 : [{best_val_score}] Best Epoch : [{best_epoch}]')\n",
    "        \n",
    "        if best_val_score == 1.0: break\n",
    "        \n",
    "        early_stop += 1\n",
    "        if early_stop > config[\"early_stop\"]: break\n",
    "    \n",
    "    with open(f\"{config['save_path']}/{config['model']}_{method}.pkl\",\"wb\") as f:\n",
    "        pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa8c900-4a7c-4ddd-a02b-71e57fc654bb",
   "metadata": {},
   "source": [
    ">- ### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0f00e36-1103-4add-a768-525299091048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iter(val_loader)):\n",
    "            videos = batch[0].to(device=device, dtype=torch.float32, non_blocking=True)\n",
    "            labels = batch[1].to(device=device, dtype=torch.long, non_blocking=True)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logit = model(videos)\n",
    "                loss = criterion(logit, labels, logit)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            preds += logit['output'].argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    _val_score = f1_score(trues, preds, average='macro')\n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aee4f0-ef9e-4ed0-8076-f01a217219f4",
   "metadata": {},
   "source": [
    ">- ### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abcbb055-9744-4ae3-a715-c9602b10f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(method, backbone):\n",
    "    num_class = {\"crash\": 2, \"ego_involve\": 2, \"weather\": 3, \"timing\": 2, \"all\": 13}\n",
    "    if method == \"crash\":\n",
    "        # model = TubeViT(num_classes=num_class[method], video_shape=[3, 50, 224, 224], num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072)\n",
    "        model = MViT(num_classes=num_class[method])\n",
    "        # config[\"model\"] == \"MViT\"\n",
    "    elif method == \"ego_involve\":\n",
    "        # model = TubeViT(num_classes=num_class[method], video_shape=[3, 50, 224, 224], num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072)\n",
    "        model = MViT(num_classes=num_class[method])\n",
    "        # config[\"model\"] == \"MViT\"\n",
    "    elif method == \"weather\":\n",
    "        # model = TubeViT(num_classes=num_class[method], video_shape=[3, 50, 224, 224], num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072)\n",
    "        model = MViT(num_classes=num_class[method])\n",
    "        # model = WeatherNet(backbone=\"deit_base_distilled_patch16_224\", num_classes=num_class[method])\n",
    "        # config[\"model\"] == \"WeatherNet\"\n",
    "    elif method == \"timing\":\n",
    "        # model = TubeViT(num_classes=num_class[method], video_shape=[3, 50, 224, 224], num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072)\n",
    "        model = MViT(num_classes=num_class[method])\n",
    "        # model = WeatherNet(backbone=\"deit_base_distilled_patch16_224\", num_classes=num_class[method])\n",
    "        # model = MViT(num_classes=num_class[method])\n",
    "    #     config[\"model\"] == \"MViT\"\n",
    "    # if config[\"model\"] == \"S3D\":\n",
    "    #     model = S3D(num_classes=num_class[method])\n",
    "    # elif config[\"model\"] == \"ResNet\":\n",
    "    #     model = ResNet(num_classes=num_class[method])\n",
    "    # elif config[\"model\"] == \"MViT\":\n",
    "    #     model = MViT(num_classes=num_class[method])\n",
    "    # elif config[\"model\"] == \"HRNet\":\n",
    "    #     model = HRNet(num_classes=num_class[method])\n",
    "    # elif config[\"model\"] == \"Swin3d\":\n",
    "    #     model = Swin3d(num_classes=num_class[method])\n",
    "    # else:\n",
    "    #     model = WeatherNet(num_classes=num_class[method])\n",
    "    model.eval()\n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "    \n",
    "    train_df, val_df = get_df(method=method)\n",
    "    train_dataset = CustomDataset(config[\"h5_name\"], method=method, phase=\"train\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"])\n",
    "\n",
    "    val_dataset = CustomDataset(config[\"h5_name\"], method=method, phase=\"val\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size = config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"])\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer=optimizer,\n",
    "        max_lr=config[\"max_lr\"],\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=config[\"epochs\"],\n",
    "    )\n",
    "    \n",
    "    train(model, optimizer, train_loader, val_loader, scheduler, config[\"device\"], method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d8fe7-2997-4266-92cf-1358aeb833dd",
   "metadata": {},
   "source": [
    ">- ### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93212d34-ea6c-4c80-852b-dc8945314f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_dict):\n",
    "    test = pd.read_csv(os.path.join(config[\"data_path\"], \"train.csv\"))\n",
    "    test_dataset = TestDataset(test['video_path'].values, test['label'].values, test['sample_id'].values)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "    \n",
    "    for value in model_dict.values():\n",
    "        value.eval()\n",
    "        \n",
    "    preds = []\n",
    "    targets = []\n",
    "    filenames = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iter(test_loader)):\n",
    "            videos = batch[0].to(device=config[\"device\"], dtype=torch.float32, non_blocking=True)\n",
    "            targets.extend(batch[1].numpy())\n",
    "            filenames.extend(batch[2])\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                crash_pred = model_dict[\"crash\"](videos).argmax(1)\n",
    "                crash_all_pred = model_dict[\"crash_all\"](videos).argmax(1)\n",
    "                # ego_involve_pred = model_dict[\"ego_involve\"](videos).argmax(1)\n",
    "                # weather_pred = model_dict[\"weather\"](videos).argmax(1)\n",
    "                # timing_pred = model_dict[\"timing\"](videos).argmax(1)\n",
    "            \n",
    "            for c, a in zip(crash_pred, crash_all_pred):\n",
    "                if c == 0:\n",
    "                    preds.append(0)\n",
    "                else:\n",
    "                    preds.append(a)\n",
    "            # for c, e, w, t in zip(crash_pred, ego_involve_pred, weather_pred, timing_pred):\n",
    "            #     if c == 0:\n",
    "            #         preds.append(0)\n",
    "            #     else:\n",
    "            #         if e == 0 and w == 0 and t == 0:\n",
    "            #             preds.append(1)\n",
    "            #         elif e == 0 and w == 0 and t == 1:\n",
    "            #             preds.append(2)\n",
    "            #         elif e == 0 and w == 1 and t == 0:\n",
    "            #             preds.append(3)\n",
    "            #         elif e == 0 and w == 1 and t == 1:\n",
    "            #             preds.append(4)\n",
    "            #         elif e == 0 and w == 2 and t == 0:\n",
    "            #             preds.append(5)\n",
    "            #         elif e == 0 and w == 2 and t == 1:\n",
    "            #             preds.append(6)\n",
    "            #         elif e == 1 and w == 0 and t == 0:\n",
    "            #             preds.append(7)\n",
    "            #         elif e == 1 and w == 0 and t == 1:\n",
    "            #             preds.append(8)\n",
    "            #         elif e == 1 and w == 1 and t == 0:\n",
    "            #             preds.append(9)\n",
    "            #         elif e == 1 and w == 1 and t == 1:\n",
    "            #             preds.append(10)\n",
    "            #         elif e == 1 and w == 2 and t == 0:\n",
    "            #             preds.append(11)\n",
    "            #         elif e == 1 and w == 2 and t == 1:\n",
    "            #             preds.append(12)\n",
    "\n",
    "    return preds, targets, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a6d36-2936-45ba-ad1e-774e30bca73c",
   "metadata": {},
   "source": [
    ">- ### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59ad882d-a0b0-4f56-b565-3474afc6b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for videos in tqdm(iter(test_loader)):\n",
    "            videos = videos.to(device)\n",
    "            videos = videos[:, :, -16:]\n",
    "            \n",
    "            logit = model(videos)\n",
    "\n",
    "            preds += logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae7396-dc4a-46d8-8f1b-7369abb05524",
   "metadata": {},
   "source": [
    ">- ### Run Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "869481c6-6bcd-45d9-ab47-69ccb9c833d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict():\n",
    "    test = pd.read_csv('./test.csv')\n",
    "    test_dataset = TestDataset(test['video_path'].values, None)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "    preds = inference(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4214e-ba02-4120-a3da-46f710efb5fa",
   "metadata": {},
   "source": [
    ">- ### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af6575ee-f0e8-49bb-a353-b6d97c9de14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(preds):\n",
    "    submit = pd.read_csv(os.path.join(config[\"data_path\"], \"sample_submission.csv\"))\n",
    "    submit['label'] = preds\n",
    "    submit.head()\n",
    "    submit.to_csv(f'{config[\"save_path\"]}/baseline_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558c6b6-d955-49b3-a140-6bc65ce3d0eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14282508-b904-4889-b765-0338cb560396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f1135c7ab14d79afc7fb09279670b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e6b3b27d09482fbcb2c885dca50868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 21:57:29,850 - Method: [crash] Epoch [1], Train Loss : [0.18274] Train F1: [0.90065] Val Loss : [0.15329] Val F1 : [0.98333] Best F1 : [0.9833333333333334] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5813bc04f46148bf8786734250495694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51fd93ff12c47f48eacd20ef8001d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:02:32,212 - Method: [crash] Epoch [2], Train Loss : [0.13261] Train F1: [0.92976] Val Loss : [3.32992] Val F1 : [0.46998] Best F1 : [0.9833333333333334] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1027689b83c94bdc9d15ee4445872df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8899a11ed245989a35822c32420a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:07:35,819 - Method: [crash] Epoch [3], Train Loss : [0.37642] Train F1: [0.72614] Val Loss : [1.69075] Val F1 : [0.65886] Best F1 : [0.9833333333333334] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53477e49c22a4651adc7037ff3a85935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c729664a26241b2adf117ef778d1edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:12:36,315 - Method: [crash] Epoch [4], Train Loss : [0.29566] Train F1: [0.80627] Val Loss : [0.69544] Val F1 : [0.91817] Best F1 : [0.9833333333333334] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e3c54380694a2596e7e01c59de58ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9a2038f5c847b2a0d43a4ca7d4c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:17:40,033 - Method: [crash] Epoch [5], Train Loss : [0.21713] Train F1: [0.86185] Val Loss : [0.88761] Val F1 : [0.86200] Best F1 : [0.9833333333333334] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecdcf0606f54302a7af177dd0161fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4add2ab3596a4624886117d554626c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:22:39,679 - Method: [crash] Epoch [6], Train Loss : [0.19459] Train F1: [0.88282] Val Loss : [0.56857] Val F1 : [0.91662] Best F1 : [0.9833333333333334] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422c23faa3f74253af3aa539c85a074c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cec29a669e44e6b46d9e8c8cd8d6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:27:37,676 - Method: [crash] Epoch [7], Train Loss : [0.07815] Train F1: [0.95714] Val Loss : [0.20964] Val F1 : [0.97324] Best F1 : [0.9833333333333334] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f37bd13147648369ddf05a182b3c814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f5713afbb74fb5a4c1a7cc985ca053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:32:37,569 - Method: [crash] Epoch [8], Train Loss : [0.03937] Train F1: [0.98046] Val Loss : [0.13071] Val F1 : [0.98559] Best F1 : [0.985592260985901] Best Epoch : [8]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b257f23a147840ff9753f522ad4a3abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1258a62235414e88a51c7c3e5fbbf356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:37:34,872 - Method: [crash] Epoch [9], Train Loss : [0.02186] Train F1: [0.98919] Val Loss : [0.12305] Val F1 : [0.97753] Best F1 : [0.985592260985901] Best Epoch : [8]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f0cd4a2fb34f4da4da1f32910d83d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce41bd704c7d44948b032e76fb3ef1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:42:32,576 - Method: [crash] Epoch [10], Train Loss : [0.00766] Train F1: [0.99587] Val Loss : [0.09468] Val F1 : [0.98157] Best F1 : [0.985592260985901] Best Epoch : [8]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b43e76b92d4bc899adfec41bc4c62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017d9f62f6d34a64b0c70256d2de3232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:44:15,292 - Method: [ego_involve] Epoch [1], Train Loss : [0.43110] Train F1: [0.70897] Val Loss : [1.47616] Val F1 : [0.75940] Best F1 : [0.7594035594035593] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00b48b616e4450e826397f35425f635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fff52a97494293a166cd9f0cd1e491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:45:58,526 - Method: [ego_involve] Epoch [2], Train Loss : [0.35256] Train F1: [0.76914] Val Loss : [1.86833] Val F1 : [0.31716] Best F1 : [0.7594035594035593] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1737a27a5b8546e2a951adf0bf244d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e0223dc31742ea970d7f6c20a45cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:47:40,691 - Method: [ego_involve] Epoch [3], Train Loss : [0.46726] Train F1: [0.66329] Val Loss : [1.76332] Val F1 : [0.65404] Best F1 : [0.7594035594035593] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5378989e3244d4ad5a3c0c6e34a38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0f0f65e3f74d9686a678d89aa97c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:49:22,870 - Method: [ego_involve] Epoch [4], Train Loss : [0.48441] Train F1: [0.66337] Val Loss : [1.87621] Val F1 : [0.70385] Best F1 : [0.7594035594035593] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89f002c9a3c4849bf3a80e08f24f0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861a39c6fcad4efba846eb0ccc363b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 22:51:06,935 - Method: [ego_involve] Epoch [5], Train Loss : [0.48096] Train F1: [0.65161] Val Loss : [2.12807] Val F1 : [0.32845] Best F1 : [0.7594035594035593] Best Epoch : [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79f45c3608b4deb85f01db0af0ed360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"swinv2_base_window8_256\": 0.6630420066708513\n",
    "# \"regnetz_040\": 0.7489193005322038\n",
    "# \"regnety_040\": 0.7055432963288931\n",
    "# \"regnetv_040\": 0.7563289952368452\n",
    "# 'resnetv2_50': 0.6496801796077435,\n",
    "# 'tresnet_m': 0.7588267287207852,\n",
    "# 'hrnet_w64': 0.6522576577751691,\n",
    "# 'wide_resnet50_2': 0.6562548562548562\n",
    "# \"tf_efficientnetv2_s_in21ft1k\": 0.712865681031771\n",
    "# mobilenetv2_140': 0.7344136123064748,\n",
    "#  'deit3_small_patch16_224_in21ft1k': 0.6744444444444445,\n",
    "#  'jx_nest_small': 0.6763762242485646,\n",
    "#  'twins_svt_base': 0.7335441231363767,\n",
    "#  'swsl_resnext101_32x4d': 0.7493798145972059,\n",
    "#  'swin_small_patch4_window7_224': 0.6703382689298182,\n",
    "#  'tf_efficientnet_b4_ap': 0.6841474826876287,\n",
    "#  'xcit_small_12_p8_224': 0.7360193556828488,\n",
    "# 'deit_base_distilled_patch16_224': 0.774743351886209,\n",
    "# \"deit3_base_patch16_224\": 0.7636604774535809\n",
    "# 'deit_tiny_patch16_224': 0.7077855477855478,\n",
    "# 'deit_tiny_distilled_patch16_224': 0.6552881502579692,\n",
    "# 'deit_small_patch16_224': 0.7524221632244941,\n",
    "# 'deit3_small_patch16_224': 0.6744444444444445,\n",
    "# 'deit_base_patch16_224': 0.6538752253037967,\n",
    "# 'deit3_small_patch16_224_in21ft1k': 0.7077171954052939\n",
    "#  'cait_s24_224': 0.688900438900439,\n",
    "#  'resmlp_big_24_distilled_224': 0.29243353783231085,\n",
    "#  'resnet152d': 0.6889550264550265\n",
    "# \"beit_base_patch16_224\": 0.4883536768520795\n",
    "# \"convnext_small_in22ft1k\": 0.7031188314251876\n",
    "\n",
    "for method in config[\"methods\"]:\n",
    "# for method in [\"weather\"]:\n",
    "    run(method=method, backbone=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c6994-838f-4846-9256-7cbca59b36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z\n",
    "# ## Save pickle\n",
    "# with open(f\"{config['save_path']}/{config['model']}.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(model_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9ff46-4462-48e1-ade5-22ebb4732c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load pickle\n",
    "# with open(\"MViT.pkl\",\"rb\") as f:\n",
    "#     model_dict = pickle.load(f)\n",
    "# print(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797a27a-46f3-47f7-b7fa-27cebe4ecd06",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebebc7-44c9-4e94-af82-ee5934480840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preds, targets, filenames = evaluate(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2384a2-8871-40fd-8bbb-11c8996dbc25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# error_dict = {t: {} for t in targets}\n",
    "# for p, t, f in zip(preds, targets, filenames):\n",
    "#     if p != t:\n",
    "#         print(f\"pred: {p}, target: {t}, filename: {f}\")\n",
    "#         if error_dict[t].get(p) is not None:\n",
    "#             error_dict[t][p] += 1\n",
    "#         else:\n",
    "#             error_dict[t][p] = 1\n",
    "#         cnt += 1\n",
    "        \n",
    "# f1 = f1_score(targets, preds, average='macro')\n",
    "# print(\"count:\", cnt)\n",
    "# print(\"error dict:\", error_dict)\n",
    "# print(f\"f1 score: {f1}\")\n",
    "\n",
    "# p = Path(config[\"save_path\"].joinpath(\"error_dict.txt\"))\n",
    "# p.write_text(f\"{str(error_dict)}\\n f1 score: {str(f1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c860de9-b9fd-411d-94f8-d53afc5bc022",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c584c-84ca-4c36-9d92-1b39fc96be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config[\"save_path\"] = Path(\"results/2023-03-09 18:32:24.788752\")\n",
    "# test = pd.read_csv(os.path.join(config[\"data_path\"], \"test.csv\"))\n",
    "# test_dataset = TestDataset(test['video_path'].values, None)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# pred_dict = {\"crash\": [], \"ego\": [], \"weather\": [], \"timing\": []}\n",
    "# for name, method in zip([f\"{config['model']}_crash.pkl\", f\"{config['model']}_ego_involve.pkl\", f\"{config['model']}_weather.pkl\", f\"{config['model']}_timing.pkl\"], [\"crash\", \"ego\", \"weather\", \"timing\"]):\n",
    "#     with open(config[\"save_path\"].joinpath(name),\"rb\") as f:\n",
    "#         model = pickle.load(f)\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, videos in enumerate(tqdm(iter(test_loader))):\n",
    "#             videos = videos.to(device=config[\"device\"], dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 pred = model(videos)\n",
    "                \n",
    "#                 print(f\"{i}, {method}: {pred.softmax(-1).detach().cpu().numpy()}\")\n",
    "#                 pred = pred.argmax(-1)\n",
    "#                 pred_dict[method].append(pred)\n",
    "\n",
    "# pred_list = []\n",
    "# for c, e, w, t in zip(pred_dict[\"crash\"], pred_dict[\"ego\"], pred_dict[\"weather\"], pred_dict[\"timing\"]):\n",
    "#     if c == 0:\n",
    "#         preds_list.append(0)\n",
    "#     else:\n",
    "#         if e == 0 and w == 0 and t == 0:\n",
    "#             preds_list.append(1)\n",
    "#         elif e == 0 and w == 0 and t == 1:\n",
    "#             preds_list.append(2)\n",
    "#         elif e == 0 and w == 1 and t == 0:\n",
    "#             preds_list.append(3)\n",
    "#         elif e == 0 and w == 1 and t == 1:\n",
    "#             preds_list.append(4)\n",
    "#         elif e == 0 and w == 2 and t == 0:\n",
    "#             preds_list.append(5)\n",
    "#         elif e == 0 and w == 2 and t == 1:\n",
    "#             preds_list.append(6)\n",
    "#         elif e == 1 and w == 0 and t == 0:\n",
    "#             preds_list.append(7)\n",
    "#         elif e == 1 and w == 0 and t == 1:\n",
    "#             preds_list.append(8)\n",
    "#         elif e == 1 and w == 1 and t == 0:\n",
    "#             preds_list.append(9)\n",
    "#         elif e == 1 and w == 1 and t == 1:\n",
    "#             preds_list.append(10)\n",
    "#         elif e == 1 and w == 2 and t == 0:\n",
    "#             preds_list.append(11)\n",
    "#         elif e == 1 and w == 2 and t == 1:\n",
    "#             preds_list.append(12)\n",
    "\n",
    "# submit(preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889cad95-edb8-4d5d-b6f9-d31c95ab51ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(config[\"data_path\"], \"test.csv\"))\n",
    "test_dataset = TestDataset(test['video_path'].values, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# for method in config[\"methods\"]:\n",
    "#     model_dict[method].eval()\n",
    "with open(config[\"save_path\"].joinpath(\"MViT_crash.pkl\"),\"rb\") as f:\n",
    "    crash_model = pickle.load(f)\n",
    "with open(config[\"save_path\"].joinpath(\"MViT_ego_involve.pkl\"),\"rb\") as f:\n",
    "    ego_model = pickle.load(f)\n",
    "with open(config[\"save_path\"].joinpath(\"MViT_weather.pkl\"),\"rb\") as f:\n",
    "    weather_model = pickle.load(f)\n",
    "with open(config[\"save_path\"].joinpath(\"MViT_timing.pkl\"),\"rb\") as f:\n",
    "    timing_model = pickle.load(f)\n",
    "    \n",
    "preds_list = []\n",
    "crash_list = []\n",
    "crash_all_list = []\n",
    "\n",
    "crash_model.eval()\n",
    "ego_model.eval()\n",
    "timing_model.eval()\n",
    "weather_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, videos in enumerate(tqdm(iter(test_loader))):\n",
    "        videos = videos.to(device=config[\"device\"], dtype=torch.float32, non_blocking=True)\n",
    "            \n",
    "        with torch.cuda.amp.autocast():\n",
    "            # outputs = model_dict[\"crash\"](videos).argmax(1)\n",
    "            \n",
    "            # if outputs.item() == 0:\n",
    "            #     preds_list.append(0)\n",
    "            # else:\n",
    "            #     outputs = model_dict[\"crash_all\"](videos).argmax(1)\n",
    "            #     preds_list.append(outputs.item())\n",
    "            \n",
    "            crash_pred = crash_model(videos)['output']\n",
    "            ego_involve_pred = ego_model(videos)['output']\n",
    "            weather_pred = weather_model(videos)['output']\n",
    "            timing_pred = timing_model(videos)['output']\n",
    "        \n",
    "        # print(f\"{i} crash: {crash_pred.softmax(-1).detach().cpu().numpy()}, ego: {ego_involve_pred.softmax(-1).detach().cpu().numpy()}, weather: {weather_pred.softmax(-1).detach().cpu().numpy()}, timing: {timing_pred.softmax(-1).detach().cpu().numpy()}\")\n",
    "        crash_pred = crash_pred.argmax(-1)\n",
    "        ego_involve_pred = ego_involve_pred.argmax(-1)\n",
    "        weather_pred = weather_pred.argmax(-1)\n",
    "        timing_pred = timing_pred.argmax(-1)\n",
    "        \n",
    "        for c, e, w, t in zip(crash_pred, ego_involve_pred, weather_pred, timing_pred):\n",
    "            if c == 0:\n",
    "                preds_list.append(0)\n",
    "            else:\n",
    "                if e == 0 and w == 0 and t == 0:\n",
    "                    preds_list.append(1)\n",
    "                elif e == 0 and w == 0 and t == 1:\n",
    "                    preds_list.append(2)\n",
    "                elif e == 0 and w == 1 and t == 0:\n",
    "                    preds_list.append(3)\n",
    "                elif e == 0 and w == 1 and t == 1:\n",
    "                    preds_list.append(4)\n",
    "                elif e == 0 and w == 2 and t == 0:\n",
    "                    preds_list.append(5)\n",
    "                elif e == 0 and w == 2 and t == 1:\n",
    "                    preds_list.append(6)\n",
    "                elif e == 1 and w == 0 and t == 0:\n",
    "                    preds_list.append(7)\n",
    "                elif e == 1 and w == 0 and t == 1:\n",
    "                    preds_list.append(8)\n",
    "                elif e == 1 and w == 1 and t == 0:\n",
    "                    preds_list.append(9)\n",
    "                elif e == 1 and w == 1 and t == 1:\n",
    "                    preds_list.append(10)\n",
    "                elif e == 1 and w == 2 and t == 0:\n",
    "                    preds_list.append(11)\n",
    "                elif e == 1 and w == 2 and t == 1:\n",
    "                    preds_list.append(12)\n",
    "\n",
    "submit(preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72a53d-3a80-427d-9067-7d5d06c5b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb16fa-b9e8-41df-b69e-106f96d5fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for p in preds_list:\n",
    "    if isinstance(p, torch.Tensor):\n",
    "        p = p.detach().cpu().numpy()\n",
    "    output.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689bb29-56c8-4a55-ac12-8e633ed13f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5b993-c995-449d-98f1-721e71d4c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ce12f-d64b-473f-a009-616c87a3f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(method):\n",
    "    train_df = pd.read_csv(os.path.join(config[\"data_path\"], \"train.csv\"))\n",
    "    \n",
    "    if method == \"crash\":\n",
    "        # 차량 충돌 여부 f1 90 이상\n",
    "        train_df.loc[train_df[\"label\"]!=0, \"label\"] = 1\n",
    "    \n",
    "    elif method == \"ego_involve\":\n",
    "        # 차량 충돌 연관 여부 f1 68 이상\n",
    "        train_df.drop(train_df[train_df[\"label\"]==0].index, inplace=True)\n",
    "        train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==2)|(train_df[\"label\"]==3)|(train_df[\"label\"]==4)|(train_df[\"label\"]==5)|(train_df[\"label\"]==6), \"label\"] = 0 # yes\n",
    "        train_df.loc[(train_df[\"label\"]==7)|(train_df[\"label\"]==8)|(train_df[\"label\"]==9)|(train_df[\"label\"]==10)|(train_df[\"label\"]==11)|(train_df[\"label\"]==12), \"label\"] = 1 # no\n",
    "        \n",
    "    elif method == \"weather\":\n",
    "        # 날씨 구분 f1 49 이상\n",
    "        train_df.drop(train_df[train_df[\"label\"]==0].index, inplace=True)\n",
    "        train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==2)|(train_df[\"label\"]==7)|(train_df[\"label\"]==8), \"label\"] = 0 # normal\n",
    "        train_df.loc[(train_df[\"label\"]==3)|(train_df[\"label\"]==4)|(train_df[\"label\"]==9)|(train_df[\"label\"]==10), \"label\"] = 1 # snowy\n",
    "        train_df.loc[(train_df[\"label\"]==5)|(train_df[\"label\"]==6)|(train_df[\"label\"]==11)|(train_df[\"label\"]==12), \"label\"] = 2 # rainy\n",
    "        \n",
    "    elif method == \"timing\":\n",
    "        # 낮/밤 구분 f1 90 이상\n",
    "        train_df.drop(train_df[train_df[\"label\"]==0].index, inplace=True)\n",
    "        train_df.loc[(train_df[\"label\"]==1)|(train_df[\"label\"]==3)|(train_df[\"label\"]==5)|(train_df[\"label\"]==7)|(train_df[\"label\"]==9)|(train_df[\"label\"]==11), \"label\"] = 0 # day\n",
    "        train_df.loc[(train_df[\"label\"]==2)|(train_df[\"label\"]==4)|(train_df[\"label\"]==6)|(train_df[\"label\"]==8)|(train_df[\"label\"]==10)|(train_df[\"label\"]==12), \"label\"] = 1 # night\n",
    "    \n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=config[\"seed\"], stratify=train_df['label'])\n",
    "        \n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = get_df(method=\"weather\")\n",
    "print(train_df)\n",
    "print(train_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3dea0-df7b-45a3-95f1-d11e3d0e5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4bf6d-4481-4aa8-9b30-d990d452aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mim download mmsegmentation --config fcn_hr18s_512x1024_40k_cityscapes --dest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ab4f0-9ea6-477b-ab50-304ab2ee594b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file = \"fcn_hr18s_512x1024_40k_cityscapes.py\"\n",
    "checkpoint = \"fcn_hr18s_512x1024_40k_cityscapes_20200601_014216-93db27d0.pth\"\n",
    "model = init_segmentor(config_file, checkpoint=checkpoint, device='cuda:0')\n",
    "model(torch.randn(1, 3, 224, 224).to('cuda:0'))\n",
    "# del model.init_cfg\n",
    "# model.backbone.forward(torch.randn(1, 3, 224, 224).to('cuda:0'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
