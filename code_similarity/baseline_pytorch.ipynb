{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199be930-7e11-43f4-b056-a52fc1a93dc8",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94dc6b5c-f92a-403d-a6f0-599f712dd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from itertools import combinations\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, get_scheduler\n",
    "from datasets import load_dataset, load_metric\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch_optimizer as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93315f6c-f85f-403b-8337-3fb509b7237c",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73044582-489f-4b93-b38d-6cbac9f3241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"open/preprocess_bm25.csv\"\n",
    "SAMPLE_DATA = \"open/sample_train.csv\"\n",
    "CODE_DATA_PATH = \"open/code\"\n",
    "TEST_DATA = \"open/test.csv\"\n",
    "SUBMISSION = 'open/sample_submission.csv'\n",
    "PRETRAINED_MODEL = \"michiyasunaga/LinkBERT-base\" # TODO: Large 모델 성능 실험\n",
    "NUM_LABELS = 2\n",
    "MAX_LEN = 512\n",
    "BATCH = 32\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WD = 1e-2\n",
    "SEED = 42\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.1\n",
    "OUTPUT_DIR = \"./results\"\n",
    "SAVE_MODEL = f\"{PRETRAINED_MODEL}_{datetime.datetime.now().strftime('%H:%M:%S:%m')}\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3134eae-6f58-4572-b44c-d8aa7ecf3466",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d2de9a-39b2-4fad-be77-8e1d0c4f3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"version\": 1,\n",
    "    \"formatters\": {\n",
    "        \"simple\": {\"format\": \"[%(asctime)s] %(message)s\", \"datefmt\": \"%Y-%m-%d %H:%M:%S\"},\n",
    "    },\n",
    "    \"handlers\": {\n",
    "        \"console\": {\n",
    "            \"class\": \"logging.StreamHandler\",\n",
    "            \"formatter\": \"simple\",\n",
    "            \"level\": \"INFO\",\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"class\": \"logging.FileHandler\",\n",
    "            \"filename\": f\"{datetime.datetime.now().strftime('%H:%M:%S:%m')}.log\",\n",
    "            \"formatter\": \"simple\",\n",
    "            \"level\": \"INFO\",\n",
    "        },\n",
    "    },\n",
    "    \"root\": {\"handlers\": [\"console\", \"file\"], \"level\": \"INFO\"},\n",
    "    \"loggers\": {\"parent\": {\"level\": \"INFO\"}, \"parent.child\": {\"level\": \"DEBUG\"},},\n",
    "}\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55261051-9fa6-4598-900e-06105cb9c563",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c57a3ee-1cf9-4d43-a1b3-30168b890d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = False  # True 할 시 연산속도 감소. 마지막에 고정시킬 때 사용 권장.\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8420bd1-8115-40a1-91fb-021d3d3aed7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Class & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1e8dc-c3cb-4c34-935b-469a69349253",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074118f1-869f-4e70-a441-2c3d71b56bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "def preprocess_function(examples):\n",
    "    for i in range(1, 3):\n",
    "        for j in range(len(examples[f\"code{i}\"])):\n",
    "            examples[f\"code{i}\"][j] = re.sub(r\"^#.*\", \"\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            examples[f\"code{i}\"][j] = re.sub(r'\"\"\".*?\"\"\"', \"\", examples[f\"code{i}\"][j], flags=re.S)\n",
    "            examples[f\"code{i}\"][j] = re.sub(r\"^\\n\", \"\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding=\"max_length\", max_length=MAX_LEN, truncation=True)\n",
    "    if 'similar' in examples:\n",
    "        outputs[\"labels\"] = examples[\"similar\"]\n",
    "    return outputs\n",
    "\n",
    "def save_model(save_name, model, optimizer, epoch, train_loss):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"total_epoch\": EPOCHS,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": train_loss,\n",
    "    }, f\"{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1abc2f-46df-4bba-99a6-98ef1933876d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Preprocess script to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1f6adf-8c00-4491-b767-8b3230981e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_train_dataset_from_codefolder(path):\n",
    "#     scripts_list = []\n",
    "#     problem_nums = []\n",
    "\n",
    "#     for problem_folder in tqdm(os.listdir(path)):\n",
    "#         scripts = os.listdir(os.path.join(path, problem_folder))\n",
    "#         problem_num = scripts[0].split('_')[0]\n",
    "#         for script in scripts:\n",
    "#             script_file = os.path.join(path, problem_folder, script)\n",
    "#             with open(script_file, 'r', encoding='utf-8') as file:\n",
    "#                 lines = file.read()\n",
    "#             scripts_list.append(lines)\n",
    "#         problem_nums.extend([problem_num]*len(scripts))\n",
    "\n",
    "#     df = pd.DataFrame(data = {'code':scripts_list, 'problem_num':problem_nums})\n",
    "#     logger.info(f\"Descirbe: \\n{df.describe()}\")\n",
    "#     logger.info(f\"Head: \\n{df.head()}\")\n",
    "#     logger.info(f\"Length: \\n{len(df)}\")\n",
    "\n",
    "#     df['tokens'] = df['code'].apply(tokenizer.tokenize)\n",
    "#     df['len'] = df['tokens'].apply(len)\n",
    "#     logger.info(f\"Tokens Describe: \\n{df.describe()}\")\n",
    "\n",
    "#     ndf = df[df['len'] <= 512].reset_index(drop=True)\n",
    "#     logger.info(f\"Max Length Clipping Describe: \\n{ndf.describe()}\")\n",
    "#     logger.info(\"Done!\")\n",
    "#     return ndf\n",
    "    \n",
    "# def preprocess_bm25(df, file_name=\"preprocess_bm25\"):\n",
    "#     codes = df['code'].to_list()\n",
    "#     problems = df['problem_num'].unique().tolist()\n",
    "#     problems.sort()\n",
    "\n",
    "#     tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "#     bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "#     total_positive_pairs = []\n",
    "#     total_negative_pairs = []\n",
    "\n",
    "#     for problem in tqdm(problems):\n",
    "#         solution_codes = df[df['problem_num'] == problem]['code']\n",
    "#         positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "#         solution_codes_indices = solution_codes.index.to_list()\n",
    "#         negative_pairs = []\n",
    "\n",
    "#         first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "#         negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "#         negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "#         ranking_idx = 0\n",
    "\n",
    "#         for solution_code in solution_codes:\n",
    "#             negative_solutions = []\n",
    "#             while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "#                 high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "#                 if high_score_idx not in solution_codes_indices:\n",
    "#                     negative_solutions.append(df['code'].iloc[high_score_idx])\n",
    "#                 ranking_idx += 1\n",
    "\n",
    "#             for negative_solution in negative_solutions:\n",
    "#                 negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "#         total_positive_pairs.extend(positive_pairs)\n",
    "#         total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "#     pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "#     pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "#     neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "#     neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "#     pos_label = [1]*len(pos_code1)\n",
    "#     neg_label = [0]*len(neg_code1)\n",
    "\n",
    "#     pos_code1.extend(neg_code1)\n",
    "#     total_code1 = pos_code1\n",
    "#     pos_code2.extend(neg_code2)\n",
    "#     total_code2 = pos_code2\n",
    "#     pos_label.extend(neg_label)\n",
    "#     total_label = pos_label\n",
    "#     pair_data = pd.DataFrame(data={\n",
    "#         'code1':total_code1,\n",
    "#         'code2':total_code2,\n",
    "#         'similar':total_label\n",
    "#     })\n",
    "#     pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "#     pair_data.to_csv(f'open/{file_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39818a-63bc-4ee6-bee2-7af7aabe9e39",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c892e3e1-c9f4-4244-80c7-f472e845eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, dataloader):\n",
    "    train_loss = AverageMeter()\n",
    "    model.train()\n",
    "    with tqdm(dataloader, total=len(dataloader), unit=\"batch\") as train_bar:\n",
    "        for idx, batch in enumerate(train_bar, start=1):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "            accelerator.backward(loss)\n",
    "            if (idx % GRADIENT_ACCUMULATION_STEPS == 0) or (idx == len(dataloader)):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.update(loss.item(), BATCH)\n",
    "                train_bar.set_postfix(train_loss=loss.item())\n",
    "    return train_loss.avg\n",
    "\n",
    "def valid(model, dataloader):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    val_loss = AverageMeter()\n",
    "    model.eval()\n",
    "    with tqdm(dataloader, total=len(dataloader), unit=\"batch\") as val_bar:\n",
    "        for idx, batch in enumerate(val_bar):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "            if (idx % GRADIENT_ACCUMULATION_STEPS == 0) or (idx == len(dataloader)):\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "                val_loss.update(loss.item(), BATCH)\n",
    "                val_bar.set_postfix(val_loss=loss.item())\n",
    "    accuracy = metric.compute()[\"accuracy\"]\n",
    "    return val_loss.avg, accuracy\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    pred_list = []\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).tolist()\n",
    "        pred_list.extend(predictions)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574acd8-517e-4fd2-ae11-cbe2e3b10d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Train / Val / Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ece7d25b-88cb-40d4-b1f6-9d5a888d77b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879cfc1243c54a48bc1d9edf8e59c269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70860dc331bb40a3ab6583b8619c3970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd41d4ce0fea491b96910286e96bff90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess code folder to csv\n",
    "# df = make_train_dataset_from_codefolder(CODE_DATA_PATH)\n",
    "# preprocess_bm25(df)\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=TRAIN_DATA)['train']\n",
    "dataset = dataset.shuffle(seed=SEED).select(range(40000))\n",
    "dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=['code1', 'code2', 'similar'],\n",
    "    load_from_cache_file=False,\n",
    "    batched=True\n",
    ")\n",
    "dataset = dataset.train_test_split(\n",
    "    TRAIN_TEST_SPLIT_RATIO,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset[\"test\"],\n",
    "    batch_size=BATCH,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataset = load_dataset(\"csv\", data_files=TEST_DATA)['train']\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=['pair_id', 'code1', 'code2'],\n",
    "    load_from_cache_file=True,\n",
    "    batched=True\n",
    ")\n",
    "test_dataset.set_format(\"torch\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20415d1-d28f-4e80-b338-7854505808ba",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02370cd-3cbc-433d-9dfe-d6f038a9167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    fp16=True,\n",
    "    log_with=\"wandb\"\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "optimizer = optim.Lamb(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-6,\n",
    "    weight_decay=WD\n",
    ")\n",
    "\n",
    "# scheduler = get_scheduler(\n",
    "#     name=\"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=EPOCHS*len(train_dataloader)\n",
    "# )\n",
    "\n",
    "if GRADIENT_CHECKPOINTING:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "model, optimizer, train_dataloader, val_dataloader, test_dataloader = accelerator.prepare(model, optimizer, train_dataloader, val_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006262fd-dbcd-4440-b5bc-624a7feeae5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83743c7a-ccbb-404e-bafd-7b4fb9d87d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d660a1320194b5fa8b02ffa13caa9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb70cc9ce3a4b1da7367e1c748e8a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-22 19:04:48] epoch:0/5 | train -> loss:0.1455546964144876\n",
      "[2022-05-22 19:04:48] epoch:0/5 | validation -> loss:0.1161816418170929 | acc: 0.7861328125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e153bb212ee49f4bbee707c75f26652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb3b93a47d94991884efb8c1a09226c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-22 19:22:22] epoch:1/5 | train -> loss:0.1012621762904715\n",
      "[2022-05-22 19:22:22] epoch:1/5 | validation -> loss:0.09480004012584686 | acc: 0.8359375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303ff6c2b7a1430587090796c2c70781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bcef03f421473599fb15703743c41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-22 19:39:59] epoch:2/5 | train -> loss:0.08172469950736837\n",
      "[2022-05-22 19:39:59] epoch:2/5 | validation -> loss:0.07689379900693893 | acc: 0.87109375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25843ae6278e44a9aa7b592279dbcc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = trainer(model, optimizer, train_dataloader)\n",
    "    val_loss, val_accuracy = valid(model, val_dataloader)\n",
    "    if val_accuracy > best_accuracy:\n",
    "        save_model(\"best\", model, optimizer, epoch, train_loss)\n",
    "    save_model(\"last\", model, optimizer, epoch, train_loss)\n",
    "    logger.info(f\"epoch:{epoch}/{EPOCHS} | train -> loss:{train_loss}\")\n",
    "    logger.info(f\"epoch:{epoch}/{EPOCHS} | validation -> loss:{val_loss} | acc: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93981f67-bb59-41b9-b1d7-67af59f7e393",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f82389-c08d-4b14-9b1e-d94d71ff214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, test_dataloader)\n",
    "\n",
    "df = pd.read_csv(SUBMISSION)\n",
    "df['similar'] = predictions\n",
    "df.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
