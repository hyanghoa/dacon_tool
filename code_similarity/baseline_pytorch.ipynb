{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199be930-7e11-43f4-b056-a52fc1a93dc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94dc6b5c-f92a-403d-a6f0-599f712dd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import logging.config\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from itertools import combinations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import faiss\n",
    "import deepspeed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch_optimizer as optim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_scheduler,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import load_dataset, load_metric\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import hyptorch.nn as hypnn\n",
    "from pytorch_metric_learning import miners, losses, testers\n",
    "from pytorch_metric_learning.utils.inference import InferenceModel, MatchFinder\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93315f6c-f85f-403b-8337-3fb509b7237c",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73044582-489f-4b93-b38d-6cbac9f3241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"open/bm25-graphcodebert-base-all_case_v3.csv\"\n",
    "VAL_DATA = \"open/sample_train.csv\"\n",
    "SAMPLE_DATA = \"open/sample_train.csv\"\n",
    "CODE_DATA_PATH = \"open/code\"\n",
    "TEST_DATA = \"open/test.csv\"\n",
    "SUBMISSION = 'open/sample_submission.csv'\n",
    "PRETRAINED_MODEL = \"microsoft/graphcodebert-base\"\n",
    "NUM_LABELS = 2\n",
    "DIM = 768\n",
    "MAX_LEN = 512\n",
    "BATCH = 32\n",
    "NUM_WORKERS = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "EPOCHS = 5\n",
    "MAX_LR = 5e-3\n",
    "MIN_LR = 5e-6\n",
    "WD = 1e-2\n",
    "SEED = 12361\n",
    "TRAIN_SELECT_DATA = 2000\n",
    "VAL_SELECT_DATA = int(TRAIN_SELECT_DATA*0.1)\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.1\n",
    "OUTPUT_DIR = \"./results\"\n",
    "SAVE_MODEL = f\"{PRETRAINED_MODEL}_{datetime.datetime.now().strftime('%H:%M:%S:%m')}\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ['MASTER_ADDR'] = 'localhost'\n",
    "# os.environ['MASTER_PORT'] = '29500' # modify if RuntimeError: Address already in use\n",
    "# os.environ['RANK'] = \"0\"\n",
    "# os.environ['LOCAL_RANK'] = \"0\"\n",
    "# os.environ['WORLD_SIZE'] = \"1\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3134eae-6f58-4572-b44c-d8aa7ecf3466",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d2de9a-39b2-4fad-be77-8e1d0c4f3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"version\": 1,\n",
    "    \"formatters\": {\n",
    "        \"simple\": {\"format\": \"[%(asctime)s] %(message)s\", \"datefmt\": \"%Y-%m-%d %H:%M:%S\"},\n",
    "    },\n",
    "    \"handlers\": {\n",
    "        \"console\": {\n",
    "            \"class\": \"logging.StreamHandler\",\n",
    "            \"formatter\": \"simple\",\n",
    "            \"level\": \"INFO\",\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"class\": \"logging.FileHandler\",\n",
    "            \"filename\": f\"logs/{datetime.datetime.now().strftime('%H:%M:%S:%m')}.log\",\n",
    "            \"formatter\": \"simple\",\n",
    "            \"level\": \"INFO\",\n",
    "        },\n",
    "    },\n",
    "    \"root\": {\"handlers\": [\"console\", \"file\"], \"level\": \"INFO\"},\n",
    "    \"loggers\": {\"parent\": {\"level\": \"INFO\"}, \"parent.child\": {\"level\": \"DEBUG\"},},\n",
    "}\n",
    "\n",
    "logging.config.dictConfig(config)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55261051-9fa6-4598-900e-06105cb9c563",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c57a3ee-1cf9-4d43-a1b3-30168b890d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8420bd1-8115-40a1-91fb-021d3d3aed7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Class & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1e8dc-c3cb-4c34-935b-469a69349253",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074118f1-869f-4e70-a441-2c3d71b56bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def preprocess_function(examples):\n",
    "    for i in range(1, 3):\n",
    "        for j in range(len(examples[f\"code{i}\"])):\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"#.*\", \"\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            examples[f\"code{i}\"][j] = re.sub('\"\"\".*?\"\"\"', \"\", examples[f\"code{i}\"][j], flags=re.S)\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"'''.*?'''\", \"\", examples[f\"code{i}\"][j], flags=re.S)\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"b'.*?'\", \"b''\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            examples[f\"code{i}\"][j] = re.sub('b\".*?\"', 'b\"\"', examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"^from .*? import .*?\\n\", \"\", examples[f\"code{i}\"][j], flags=re.MULTILINE) # TODO: 이거 포함시켜서 preprocess 하면 성능 향상 되는지 확인하기\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"^import .*?\\n\", \"\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"@.*\", \"\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"^\\n\", \"\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"^ *?\\n\", \"\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            examples[f\"code{i}\"][j] = re.sub(\"    \", \"\\t\", examples[f\"code{i}\"][j], flags=re.MULTILINE)\n",
    "            \n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding=\"max_length\", max_length=MAX_LEN, truncation=True)\n",
    "    if 'similar' in examples:\n",
    "        outputs[\"labels\"] = examples[\"similar\"]\n",
    "    return outputs\n",
    "\n",
    "def save_model(save_name, model, optimizer, epoch, train_loss):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"total_epoch\": EPOCHS,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": train_loss,\n",
    "    }, f\"{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1abc2f-46df-4bba-99a6-98ef1933876d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Preprocess script to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1f6adf-8c00-4491-b767-8b3230981e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_train_dataset_from_codefolder(path):\n",
    "#     scripts_list = []\n",
    "#     problem_nums = []\n",
    "\n",
    "#     for problem_folder in tqdm(os.listdir(path)):\n",
    "#         scripts = os.listdir(os.path.join(path, problem_folder))\n",
    "#         problem_num = scripts[0].split('_')[0]\n",
    "#         for script in scripts:\n",
    "#             script_file = os.path.join(path, problem_folder, script)\n",
    "#             with open(script_file, 'r', encoding='utf-8') as file:\n",
    "#                 lines = file.read()\n",
    "#             lines = re.sub(r\"#.*\", \"\", lines, flags=re.MULTILINE)\n",
    "#             lines = re.sub(r'\"\"\".*?\"\"\"', \"\", lines, flags=re.S)\n",
    "#             lines = re.sub(r\"^\\n\", \"\", lines, flags=re.MULTILINE)\n",
    "#             lines = re.sub(r\"^ *?\\n\", \"\", lines, flags=re.MULTILINE)\n",
    "#             lines = re.sub(r\"    \", \"\\t\", lines, flags=re.MULTILINE)\n",
    "#             scripts_list.append(lines)\n",
    "#         problem_nums.extend([problem_num]*len(scripts))\n",
    "\n",
    "#     df = pd.DataFrame(data = {'code':scripts_list, 'problem_num':problem_nums})\n",
    "#     logger.info(f\"Descirbe: \\n{df.describe()}\")\n",
    "#     logger.info(f\"Head: \\n{df.head()}\")\n",
    "#     logger.info(f\"Length: \\n{len(df)}\")\n",
    "\n",
    "#     df['tokens'] = df['code'].apply(tokenizer.tokenize)\n",
    "#     df['len'] = df['tokens'].apply(len)\n",
    "#     logger.info(f\"Tokens Describe: \\n{df.describe()}\")\n",
    "\n",
    "#     ndf = df[df['len'] <= 512].reset_index(drop=True)\n",
    "#     logger.info(f\"Max Length Clipping Describe: \\n{ndf.describe()}\")\n",
    "\n",
    "#     train_df, valid_df, train_label, valid_label = train_test_split(\n",
    "#         ndf,\n",
    "#         ndf['problem_num'],\n",
    "#         random_state=42,\n",
    "#         test_size=0.1,\n",
    "#         stratify=ndf['problem_num'],\n",
    "#     )\n",
    "\n",
    "#     train_df = train_df.reset_index(drop=True)\n",
    "#     valid_df = valid_df.reset_index(drop=True)\n",
    "#     logger.info(\"Done!\")\n",
    "#     return train_df, valid_df\n",
    "    \n",
    "# # def preprocess_bm25(df, file_name=\"preprocess_bm25\"):\n",
    "# #     codes = df['code'].to_list()\n",
    "# #     problems = df['problem_num'].unique().tolist()\n",
    "# #     problems.sort()\n",
    "\n",
    "# #     tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "# #     bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# #     total_positive_pairs = []\n",
    "# #     total_negative_pairs = []\n",
    "\n",
    "# #     for problem in tqdm(problems):\n",
    "# #         solution_codes = df[df['problem_num'] == problem]['code']\n",
    "# #         positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "# #         solution_codes_indices = solution_codes.index.to_list()\n",
    "# #         negative_pairs = []\n",
    "\n",
    "# #         first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "# #         negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "# #         negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "# #         ranking_idx = 0\n",
    "\n",
    "# #         for solution_code in solution_codes:\n",
    "# #             negative_solutions = []\n",
    "# #             while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "# #                 high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "# #                 if high_score_idx not in solution_codes_indices:\n",
    "# #                     negative_solutions.append(df['code'].iloc[high_score_idx])\n",
    "# #                 ranking_idx += 1\n",
    "\n",
    "# #             for negative_solution in negative_solutions:\n",
    "# #                 negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "# #         total_positive_pairs.extend(positive_pairs)\n",
    "# #         total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "# #     pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "# #     pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "# #     neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "# #     neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "# #     pos_label = [1]*len(pos_code1)\n",
    "# #     neg_label = [0]*len(neg_code1)\n",
    "\n",
    "# #     pos_code1.extend(neg_code1)\n",
    "# #     total_code1 = pos_code1\n",
    "# #     pos_code2.extend(neg_code2)\n",
    "# #     total_code2 = pos_code2\n",
    "# #     pos_label.extend(neg_label)\n",
    "# #     total_label = pos_label\n",
    "# #     pair_data = pd.DataFrame(data={\n",
    "# #         'code1':total_code1,\n",
    "# #         'code2':total_code2,\n",
    "# #         'similar':total_label\n",
    "# #     })\n",
    "# #     pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "# #     pair_data.to_csv(f'open/{file_name}.csv',index=False)\n",
    "\n",
    "\n",
    "# def preprocess_bm25(df, file_name=\"preprocess_bm25\"):\n",
    "#     problems = sorted(df['problem_num'].unique().tolist())\n",
    "#     positive_pairs = []\n",
    "#     negative_pairs = []\n",
    "#     for problem in tqdm(problems):\n",
    "#         positive_codes = df[df[\"problem_num\"]==problem][\"code\"].to_list()\n",
    "#         # negative_codes = df[df[\"problem_num\"]!=problem][\"code\"].to_list()\n",
    "#         positive_tokenized_corpus = [tokenizer.tokenize(code) for code in positive_codes]\n",
    "#         # negative_tokenized_corpus = [tokenizer.tokenize(code) for code in negative_codes]\n",
    "#         positive_bm25 = BM25Okapi(positive_tokenized_corpus)\n",
    "#         # negative_bm25 = BM25Okapi(negative_tokenized_corpus)\n",
    "\n",
    "#         # get positive_pairs\n",
    "#         for idx, code in enumerate(positive_codes, start=1):\n",
    "#             tokenized_code = tokenizer.tokenize(code)\n",
    "#             positive_scores = positive_bm25.get_scores(tokenized_code)\n",
    "#             # negative_scores = negative_bm25.get_scores(tokenized_code)\n",
    "            \n",
    "#             for _ in range(2):\n",
    "#                 for i in range(len(positive_scores)):\n",
    "#                     positive_bottom = positive_scores.argsort()[i]\n",
    "#                     if (code, positive_bottom) not in positive_pairs and (positive_bottom, code) not in positive_pairs:\n",
    "#                         positive_pairs.append((code, positive_bottom))\n",
    "#                         break\n",
    "            \n",
    "#             if idx == len(positive_codes):\n",
    "#                 for p in problems:\n",
    "#                     if problem == p:\n",
    "#                         continue\n",
    "#                     negative_codes = df[df[\"problem_num\"]==p][\"code\"].to_list()\n",
    "#                     for negative_code in negative_codes:\n",
    "#                         if (code, negative_code) not in negative_pairs and (negative_code, code) not in negative_pairs:\n",
    "#                             negative_pairs.append((code, negative_code))\n",
    "#                             break\n",
    "            \n",
    "#         # print(\"positive:\", len(positive_pairs), \"negative:\", len(negative_pairs))\n",
    "#             # for i in range(len(negative_scores)):\n",
    "#             #     negative_bottom = negative_scores.argsort()[i]\n",
    "#             #     if (code, negative_bottom) not in negative_pairs and (negative_bottom, code) not in negative_pairs:\n",
    "#             #         negative_pairs.append((code, negative_bottom))\n",
    "#             #         break\n",
    "\n",
    "#     positive_labels = [1]*len(positive_pairs)\n",
    "#     negative_labels = [0]*len(negative_pairs)\n",
    "\n",
    "#     total_pairs = []\n",
    "#     total_labels = []\n",
    "#     total_pairs.extend(positive_pairs)\n",
    "#     total_pairs.extend(negative_pairs)\n",
    "#     total_labels.extend(positive_labels)\n",
    "#     total_labels.extend(negative_labels)\n",
    "#     total_code1 = list(map(lambda x:x[0], total_pairs))\n",
    "#     total_code2 = list(map(lambda x:x[1], total_pairs))\n",
    "        \n",
    "#     pair_data = pd.DataFrame(data={\n",
    "#         'code1':total_code1,\n",
    "#         'code2':total_code2,\n",
    "#         'similar':total_labels\n",
    "#     })\n",
    "#     pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "#     pair_data.to_csv(f'{file_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf8fbcd-e150-41b3-a146-67842af77a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, val_df = make_train_dataset_from_codefolder(CODE_DATA_PATH)\n",
    "# preprocess_bm25(train_df, file_name=\"preprocess_bm25_train\")\n",
    "# preprocess_bm25(val_df, file_name=\"preprocess_bm25_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39818a-63bc-4ee6-bee2-7af7aabe9e39",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c892e3e1-c9f4-4244-80c7-f472e845eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    train_loss = AverageMeter()\n",
    "    model.train()\n",
    "    with tqdm(dataloader, total=len(dataloader), unit=\"batch\") as train_bar:\n",
    "        for idx, batch in enumerate(train_bar, start=1):\n",
    "            embeddings = model(batch)\n",
    "            # hard_pairs = miner(embeddings, batch[\"labels\"])\n",
    "            # loss = loss_func(embeddings, batch[\"labels\"], hard_pairs)\n",
    "            loss = loss_func(embeddings, batch[\"labels\"])\n",
    "            # loss = sce_loss(embeddings, batch[\"labels\"])\n",
    "            loss /= GRADIENT_ACCUMULATION_STEPS\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            if (idx % GRADIENT_ACCUMULATION_STEPS == 0) or (idx == len(dataloader)):\n",
    "                accelerator.clip_grad_norm_(model.parameters(), max_norm=1, norm_type=2)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                predictions = torch.argmax(loss_func.get_logits(embeddings), dim=-1)\n",
    "                # predictions = torch.argmax(embeddings, dim=-1)\n",
    "                metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "                train_loss.update(loss.item(), BATCH)\n",
    "                train_bar.set_postfix(train_loss=loss.item(), train_acc=metric.compute()[\"accuracy\"])\n",
    "            \n",
    "            ########## DEEPSPEED #############\n",
    "            # if (idx % GRADIENT_ACCUMULATION_STEPS == 0) or (idx == len(dataloader)):\n",
    "            # model.backward(loss)\n",
    "            # model.step()\n",
    "            # train_loss.update(loss.item(), BATCH)\n",
    "            # train_bar.set_postfix(train_loss=loss.item())\n",
    "            ########## DEEPSPEED #############\n",
    "    return train_loss.avg\n",
    "\n",
    "def valid(model, dataloader):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    val_loss = AverageMeter()\n",
    "    model.eval()\n",
    "    with tqdm(dataloader, total=len(dataloader), unit=\"batch\") as val_bar:\n",
    "        for idx, batch in enumerate(val_bar):\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(batch)\n",
    "            loss = loss_func(embeddings, batch[\"labels\"])\n",
    "            # loss = sce_loss(embeddings, batch[\"labels\"])\n",
    "            predictions = torch.argmax(loss_func.get_logits(embeddings), dim=-1)\n",
    "            # predictions = torch.argmax(embeddings, dim=-1)\n",
    "            metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "            val_loss.update(loss.item(), BATCH)\n",
    "            val_bar.set_postfix(val_loss=loss.item())\n",
    "\n",
    "    return val_loss.avg, metric.compute()[\"accuracy\"]\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    pred_list = []\n",
    "    model.eval()\n",
    "    for batch in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(batch)\n",
    "        predictions = torch.argmax(loss_func.get_logits(embeddings), dim=-1).tolist()\n",
    "        pred_list.extend(predictions)\n",
    "    return pred_list\n",
    "\n",
    "# def valid_kmeans(model, dataloader):\n",
    "#     metric = load_metric(\"accuracy\")\n",
    "#     val_loss = AverageMeter()\n",
    "#     kmeans = MiniBatchKMeans(n_clusters=2, random_state=SEED)\n",
    "#     embeds = []\n",
    "#     labels = []\n",
    "#     # tmp = []\n",
    "#     # match_finder = MatchFinder(distance=CosineSimilarity(), threshold=0.7)\n",
    "#     # inference_model = InferenceModel(model, match_finder=match_finder, normalize_embeddings=False)\n",
    "#     model.eval()\n",
    "    \n",
    "    \n",
    "#     ###########################################################\n",
    "#     # tmp = []\n",
    "#     # for batch in trainloader:\n",
    "#     #     with torch.no_grad():\n",
    "#     #         outputs = model(batch)\n",
    "#     #     tmp.extend(outputs[\"embed\"].cpu().numpy())\n",
    "#     # kmeans.fit(tmp)\n",
    "#     ###########################################################\n",
    "    \n",
    "    \n",
    "#     with tqdm(dataloader, total=len(dataloader), unit=\"batch\") as val_bar:\n",
    "#         for idx, batch in enumerate(val_bar):\n",
    "#             with torch.no_grad():\n",
    "#                 # outputs = model(**batch)\n",
    "#                 outputs = model(batch)\n",
    "#                 # decision = inference_model.is_match(batch, batch[\"labels\"])\n",
    "#             # tmp.extend(decision)\n",
    "#             loss = loss_func(outputs, batch[\"labels\"])\n",
    "#             embeds.extend(outputs.detach().cpu().numpy())\n",
    "#             labels.extend(batch[\"labels\"])\n",
    "#             val_loss.update(loss.item(), BATCH)\n",
    "#             val_bar.set_postfix(val_loss=loss.item())\n",
    "#         klabel = kmeans.fit_predict(embeds)\n",
    "#         # print(tmp)\n",
    "#         # print(\"acc:\", np.sum(tmp) / len(tmp))\n",
    "#         metric.add_batch(predictions=klabel, references=labels)\n",
    "#     return val_loss.avg, metric.compute()[\"accuracy\"]\n",
    "\n",
    "# def predict_kmeans(model, dataloader):\n",
    "#     kmeans = MiniBatchKMeans(n_clusters=2, random_state=SEED)\n",
    "#     embeds = []\n",
    "#     model.eval()\n",
    "#     for batch in tqdm(dataloader):\n",
    "#         # batch = {k: v.to(\"cuda:0\") for k, v in batch.items()}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.predict(batch)\n",
    "#         embeds.extend(outputs[\"embed\"].detach().cpu().numpy())\n",
    "#     klabel = kmeans.fit_predict(embeds)\n",
    "#     return klabel\n",
    "\n",
    "\n",
    "# def valid_metric(model, train_dataloader, val_dataloader):\n",
    "#     metric = load_metric(\"accuracy\")\n",
    "#     val_loss = AverageMeter()\n",
    "    \n",
    "#     train_labels = []\n",
    "#     predictions = []\n",
    "#     index = faiss.IndexFlatL2(DIM)\n",
    "\n",
    "#     model.eval()\n",
    "#     for batch in tqdm(train_dataloader):\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(batch)\n",
    "#         index.add(outputs[\"embed\"].cpu().numpy())\n",
    "#         train_labels.extend(batch[\"labels\"])\n",
    "    \n",
    "#     with tqdm(val_dataloader, total=len(val_dataloader), unit=\"batch\") as val_bar:\n",
    "#         for idx, batch in enumerate(val_bar):\n",
    "#             predictions = []\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model(batch)\n",
    "#             d, i = index.search(outputs[\"embed\"].cpu().numpy(), 3)\n",
    "#             for idxs in i:\n",
    "#                 prediction = [train_labels[idx].item() for idx in idxs]\n",
    "#                 num_0 = prediction.count(0)\n",
    "#                 num_1 = prediction.count(1)\n",
    "#                 if num_0 > num_1:\n",
    "#                     predictions.extend([0])\n",
    "#                 else:\n",
    "#                     predictions.extend([1])\n",
    "\n",
    "#             # i = i.squeeze()\n",
    "#             # prediction = [train_labels[idx].item() for idx in i]\n",
    "            \n",
    "#             metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "#             val_loss.update(outputs[\"loss\"], BATCH)\n",
    "#             val_bar.set_postfix(val_loss=outputs[\"loss\"].item())\n",
    "\n",
    "#     return val_loss.avg, metric.compute()[\"accuracy\"]\n",
    "\n",
    "# def predict_metric(model, train_dataloader, test_dataloader):\n",
    "#     metric = load_metric(\"accuracy\")\n",
    "#     val_loss = AverageMeter()\n",
    "    \n",
    "#     train_labels = []\n",
    "#     predictions = []\n",
    "#     index = faiss.IndexFlatL2(DIM)\n",
    "\n",
    "#     model.eval()\n",
    "#     for batch in tqdm(train_dataloader):\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(batch)\n",
    "#         index.add(outputs[\"embed\"].cpu().numpy())\n",
    "#         train_labels.extend(batch[\"labels\"])\n",
    "    \n",
    "#     for batch in tqdm(test_dataloader):\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.predict(batch)\n",
    "#         d, i = index.search(outputs[\"embed\"].cpu().numpy(), 3)\n",
    "#         for idxs in i:\n",
    "#             prediction = [train_labels[idx].item() for idx in idxs]\n",
    "#             num_0 = prediction.count(0)\n",
    "#             num_1 = prediction.count(1)\n",
    "#             if num_0 > num_1:\n",
    "#                 predictions.extend([0])\n",
    "#             else:\n",
    "#                 predictions.extend([1])\n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574acd8-517e-4fd2-ae11-cbe2e3b10d33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Train / Val / Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece7d25b-88cb-40d4-b1f6-9d5a888d77b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/djlee/.cache/huggingface/datasets/csv/default-cfaa861a98c64ab5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe34a8c13e234880a06619cb5f4a8403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73cafec692446ae917b08d527c98b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/djlee/.cache/huggingface/datasets/csv/default-cfaa861a98c64ab5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624934755f944a97813a8d49b417031e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb2acaa5d294a319eaedc6feb09516a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2691 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4ab31405194d6685c11b556cacc2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1164634d2444b99f0b12b1373dbaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "tokenizer.truncation_side = \"left\"\n",
    "train_dataset = load_dataset(\"csv\", data_files=TRAIN_DATA)['train']\n",
    "# train_dataset = train_dataset.select(range(TRAIN_SELECT_DATA))\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=['code1', 'code2', 'similar'],\n",
    "    load_from_cache_file=False,\n",
    "    batched=True\n",
    ")\n",
    "train_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "val_dataset = load_dataset(\"csv\", data_files=VAL_DATA)['train']\n",
    "# val_dataset = val_dataset.select(range(VAL_SELECT_DATA))\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=['code1', 'code2', 'similar'],\n",
    "    load_from_cache_file=False,\n",
    "    batched=True\n",
    ")\n",
    "val_dataset.set_format(\"torch\")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20415d1-d28f-4e80-b338-7854505808ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4f3a27b-d92a-44c4-b54e-6307398dfc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Network(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Network, self).__init__()\n",
    "#         self.pretrained_model = AutoModel.from_pretrained(PRETRAINED_MODEL)\n",
    "#         self.embedding = nn.Linear(self.pretrained_model.config.hidden_size, DIM)\n",
    "#         if GRADIENT_CHECKPOINTING:\n",
    "#             self.pretrained_model.gradient_checkpointing_enable()\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         outputs = self.get_embeddings(inputs)\n",
    "#         hard_pairs = miner(outputs, inputs[\"labels\"])\n",
    "#         loss = loss_func(outputs, inputs[\"labels\"], hard_pairs)\n",
    "#         return {\"embed\": outputs, \"loss\": loss}\n",
    "    \n",
    "#     def predict(self, inputs):\n",
    "#         outputs = self.get_embeddings(inputs)\n",
    "#         return {\"embed\": outputs}\n",
    "    \n",
    "#     def get_embeddings(self, inputs):\n",
    "#         outputs = self.pretrained_model(\n",
    "#             input_ids=inputs.get(\"input_ids\"),\n",
    "#             token_type_ids=inputs.get(\"token_type_ids\"),\n",
    "#             attention_mask=inputs.get(\"attention_mask\")\n",
    "#         )\n",
    "#         outputs = self.embedding(outputs[\"pooler_output\"])\n",
    "#         outputs = F.normalize(outputs)\n",
    "#         return outputs\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.pretrained_model = AutoModel.from_pretrained(PRETRAINED_MODEL)\n",
    "        if GRADIENT_CHECKPOINTING:\n",
    "            self.pretrained_model.gradient_checkpointing_enable()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.pretrained_model(\n",
    "            input_ids=inputs.get(\"input_ids\"),\n",
    "            token_type_ids=inputs.get(\"token_type_ids\"),\n",
    "            attention_mask=inputs.get(\"attention_mask\")\n",
    "        )\n",
    "        embeddings = mean_pooling(embeddings, inputs.get(\"attention_mask\"))\n",
    "        embeddings = self.l2_norm(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def l2_norm(self, inputs):\n",
    "        inputs_size = inputs.size()\n",
    "        buffer = torch.pow(inputs, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-12)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _outputs = torch.div(inputs, norm.view(-1, 1).expand_as(inputs))\n",
    "\n",
    "        outputs = _outputs.view(inputs_size)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b02370cd-3cbc-433d-9dfe-d6f038a9167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS)\n",
    "accelerator = Accelerator(\n",
    "    fp16=True,\n",
    "    # deepspeed_plugin=deepspeed_plugin,\n",
    ")\n",
    "\n",
    "model = Network()\n",
    "    \n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "optimizer = optim.Lamb(\n",
    "    model.parameters(),\n",
    "    lr=MIN_LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-6,\n",
    "    weight_decay=WD\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=MAX_LR,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# TEST\n",
    "# miner = miners.MultiSimilarityMiner()\n",
    "# miner = miners.BatchEasyHardMiner()\n",
    "# miner = miners.TripletMarginMiner(margin=0.01, type_of_triplets=\"semihard\")\n",
    "# loss_func = losses.SoftTripleLoss(\n",
    "#     num_classes=NUM_LABELS,\n",
    "#     embedding_size=DIM,\n",
    "#     centers_per_class=10,\n",
    "#     la=20,\n",
    "#     gamma=0.1,\n",
    "#     margin=0.01,\n",
    "# )\n",
    "loss_func = losses.ProxyAnchorLoss(\n",
    "    num_classes=NUM_LABELS,\n",
    "    embedding_size=DIM,\n",
    ")\n",
    "\n",
    "# scheduler = get_scheduler(\n",
    "#     name=\"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=EPOCHS*len(train_dataloader)\n",
    "# )\n",
    "\n",
    "# config =  {\n",
    "#     \"train_batch_size\": BATCH,\n",
    "#     # \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "#     \"optimizer\": {\n",
    "#         \"type\": \"Adam\",\n",
    "#         \"params\": {\n",
    "#             \"lr\": LR,\n",
    "#             \"weight_decay\": WD\n",
    "#         }\n",
    "#     },\n",
    "#     \"fp16\": {\n",
    "#         \"enabled\": True\n",
    "#     },\n",
    "#     \"zero_optimization\": True\n",
    "# }\n",
    "\n",
    "# model, optimizer, _, _ = deepspeed.initialize(model=model,\n",
    "#                                               config_params=config,\n",
    "#                                               model_parameters=model.parameters())\n",
    "\n",
    "model, optimizer, loss_func, scheduler, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "    model, optimizer, loss_func, scheduler, train_dataloader, val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006262fd-dbcd-4440-b5bc-624a7feeae5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83743c7a-ccbb-404e-bafd-7b4fb9d87d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2565d6d11d524a51a6f1ca06ce375a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84094 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6e14dbf1fe40299e595cbdbe48643a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/562 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-10 13:33:19] epoch:1/5 | train -> loss: 0.6394323408846555\n",
      "[2022-06-10 13:33:19] epoch:1/5 | validation -> loss: 2.434509139617796 | acc: 0.9939343350027824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da79d8cd59fc422f9065dedadac7a393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84094 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m best_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m valid(model, val_dataloader)\n\u001b[1;32m      5\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mwait_for_everyone()\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# loss = sce_loss(embeddings, batch[\"labels\"])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m GRADIENT_ACCUMULATION_STEPS\n\u001b[0;32m---> 13\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m%\u001b[39m GRADIENT_ACCUMULATION_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)):\n\u001b[1;32m     16\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, norm_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/deep/djlee_env/lib/python3.8/site-packages/accelerate/accelerator.py:618\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    620\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/deep/djlee_env/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep/djlee_env/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    val_loss, val_accuracy = valid(model, val_dataloader)\n",
    "    accelerator.wait_for_everyone()\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        save_model(\"results_accelerator/best\", model, optimizer, epoch, train_loss)\n",
    "        # unwrapped_model = accelerator.unwrap_model(model)\n",
    "        # unwrapped_model.save_pretrained(\"./results_accelerator/best\", save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n",
    "    \n",
    "    save_model(f\"results_accelerator/epoch{epoch}\", model, optimizer, epoch, train_loss)\n",
    "    # unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # unwrapped_model.save_pretrained(f\"./results_accelerator/epoch{epoch}\", save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))\n",
    "    logger.info(f\"epoch:{epoch}/{EPOCHS} | train -> loss: {train_loss}\")\n",
    "    logger.info(\n",
    "        f\"epoch:{epoch}/{EPOCHS} | validation -> loss: {val_loss} | acc: {val_accuracy}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93981f67-bb59-41b9-b1d7-67af59f7e393",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f82389-c08d-4b14-9b1e-d94d71ff214b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1b936dc2914c04898aeea7d8b11b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac21f34d8039431b8cd861388f431af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97570f3a2b2548d3bc079d836ac970a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"csv\", data_files=TEST_DATA)['train']\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"pair_id\", 'code1', 'code2'],\n",
    "    load_from_cache_file=False, # TODO: 변경시 False\n",
    "    batched=True\n",
    ")\n",
    "test_dataset.set_format(\"torch\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_dataloader = accelerator.prepare(test_dataloader)\n",
    "checkpoint = torch.load(\"results_accelerator/best.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "predictions = predict(model, test_dataloader)\n",
    "\n",
    "df = pd.read_csv(SUBMISSION)\n",
    "df['similar'] = predictions\n",
    "df.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc8ee5-a52e-404d-a4b5-60bb038f121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
